# pylint: disable=too-many-lines, import-error
import base64
import copy
import datetime as dt

from arc_market.idx.datacamp.proto.api import ExportMessage_pb2
import dateutil as du
from google.protobuf import json_format
import psycopg2.tz
import pytest

from ... import utils

BLACK_LIST_BRAND_ID = 778
S3_AVAILABILITY_PATH = 'availability/availability_1.json'
S3_OLD_AVAILABILITY_PATH = 'old_availability/availability_1.json'
QUEUE_NAME = 'eats_nomenclature_update_availability'
PLACE_UPDATE_STATUS_PREFIX = 'availability'
AVAILABILITY_TASK_TYPE = 'availability'
MOCK_NOW = '2020-02-24T13:00:00+00:00'
OLD_FORCE_UNAVAILABLE_UNTIL = '2017-01-01T01:00:00+00:00'
TEST_DATETIME = '2021-03-01T10:45:00+03:00'
TEST_DATETIME_2 = '2021-03-01T11:45:00+03:00'
TEST_DATETIME_3 = '2021-03-01T09:45:00+03:00'


def settings(
        max_retries_on_error=3,
        max_retries_on_busy=3,
        max_busy_time_in_ms=100000,
        retry_on_busy_delay_ms=1000,
):
    return {
        '__default__': {
            'max_retries_on_error': max_retries_on_error,
            'max_retries_on_busy': max_retries_on_busy,
            'max_busy_time_in_ms': max_busy_time_in_ms,
            'retry_on_busy_delay_ms': retry_on_busy_delay_ms,
            'insert_batch_size': 1000,
            'lookup_batch_size': 1000,
        },
    }


@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_available_change_in_db(
        pgsql,
        assert_availability_file_in_s3_and_db,
        save_and_update_availability,
        duplicate_assortment_data,
        sql_get_place_processing_last_status,
        sql_get_place_processing_last_status_history,
        sql_add_place_product,
):
    place_id = 1
    old_brand_id = 888
    brand_id = 777
    now = dt.datetime.fromisoformat(MOCK_NOW)

    def sql_update_available_from(item_id, available_from):
        cursor = pgsql['eats_nomenclature'].cursor()
        cursor.execute(
            f"""update eats_nomenclature.places_products
                set available_from = '{available_from}'
                where origin_id = '{item_id}'
            """,
        )

    task_id = 1

    # Check that availability is updated for place assortments
    # of all assortment_traits.
    duplicate_assortment_data(3, 1)

    sql_update_available_from('item_origin_2', now + dt.timedelta(1))
    sql_update_available_from('item_origin_5', now)

    # Add product with the same origin_id and place_id but different brand_id.
    sql_add_place_product('item_origin_1', place_id, old_brand_id)

    fixed_values = {
        # Removed non-fixed times generated by `now()`
        ('item_origin_1', None, brand_id),
        ('item_origin_1', None, old_brand_id),
        ('item_origin_2', now + dt.timedelta(1), brand_id),
        (
            'item_origin_3',
            dt.datetime(2017, 1, 8, 4, 5, 6, tzinfo=du.tz.tzlocal()),
            brand_id,
        ),
        (
            'item_origin_4',
            dt.datetime(2037, 1, 8, tzinfo=du.tz.tzlocal()),
            brand_id,
        ),
        ('item_origin_5', now, brand_id),
    }

    db_values = sql_get_availabilities(pgsql)
    assert fixed_values == db_values

    # enqueue

    new_availabilities = [
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_5', 'available': False},
    ]

    await save_and_update_availability(
        data=new_availabilities,
        place_id=place_id,
        datetime=TEST_DATETIME,
        task_id=task_id,
    )
    task_id += 1

    db_values = sql_get_availabilities(pgsql)
    assert db_values == {
        (
            'item_origin_1',
            dt.datetime(
                2020,
                2,
                24,
                16,
                0,
                tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=180, name=None),
            ),
            brand_id,
        ),
        ('item_origin_1', None, old_brand_id),
        ('item_origin_2', None, brand_id),
        (
            'item_origin_3',
            dt.datetime(
                2017,
                1,
                8,
                4,
                5,
                6,
                tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=180, name=None),
            ),
            brand_id,
        ),
        (
            'item_origin_4',
            dt.datetime(
                2037,
                1,
                8,
                0,
                0,
                tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=180, name=None),
            ),
            brand_id,
        ),
        ('item_origin_5', None, brand_id),
    }
    assert_availability_file_in_s3_and_db(
        place_id, S3_OLD_AVAILABILITY_PATH, TEST_DATETIME, new_availabilities,
    )

    # enqueue a newer file
    new_availabilities_2 = copy.deepcopy(new_availabilities)
    new_availabilities_2[0]['available'] = False

    await save_and_update_availability(
        data=new_availabilities_2,
        place_id=place_id,
        datetime=TEST_DATETIME_2,
        task_id=task_id,
    )
    task_id += 1

    db_values = sql_get_availabilities(pgsql)
    last_expected_availabilities = {
        ('item_origin_1', None, brand_id),
        ('item_origin_1', None, old_brand_id),
        ('item_origin_2', None, brand_id),
        (
            'item_origin_3',
            dt.datetime(
                2017,
                1,
                8,
                4,
                5,
                6,
                tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=180, name=None),
            ),
            brand_id,
        ),
        (
            'item_origin_4',
            dt.datetime(
                2037,
                1,
                8,
                0,
                0,
                tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=180, name=None),
            ),
            brand_id,
        ),
        ('item_origin_5', None, brand_id),
    }
    assert last_expected_availabilities == db_values
    assert_availability_file_in_s3_and_db(
        place_id,
        S3_OLD_AVAILABILITY_PATH,
        TEST_DATETIME_2,
        new_availabilities_2,
    )

    # enqueue an older file
    new_availabilities_3 = copy.deepcopy(new_availabilities)
    new_availabilities_3[0]['available'] = False
    new_availabilities_3[1]['available'] = False

    await save_and_update_availability(
        data=new_availabilities_3,
        place_id=place_id,
        datetime=TEST_DATETIME_3,
        task_id=task_id,
    )
    task_id += 1

    # availability and last processed file are the same as in the previous case
    db_values = sql_get_availabilities(pgsql)
    assert last_expected_availabilities == db_values
    assert_availability_file_in_s3_and_db(
        place_id,
        S3_OLD_AVAILABILITY_PATH,
        TEST_DATETIME_2,
        new_availabilities_2,
    )

    last_status = {
        'status': 'processed',
        'task_error': None,
        'task_error_details': None,
        'task_warnings': None,
        'status_or_text_changed_at': now,
    }
    expected_place_proc_statuses = [last_status]
    assert (
        sql_get_place_processing_last_status(place_id, AVAILABILITY_TASK_TYPE)
        == last_status
    )

    assert (
        sql_get_place_processing_last_status_history(
            place_id, AVAILABILITY_TASK_TYPE,
        )
        == expected_place_proc_statuses
    )


@pytest.mark.parametrize(**utils.gen_bool_params('use_stocks_stq'))
@pytest.mark.parametrize(**utils.gen_bool_params('change_availability'))
@pytest.mark.parametrize(
    **utils.gen_bool_params('has_active_place_categories'),
)
@pytest.mark.parametrize(
    **utils.gen_bool_params('has_force_unavailability_just_started'),
)
@pytest.mark.parametrize(
    **utils.gen_bool_params('has_force_unavailability_just_ended'),
)
@pytest.mark.parametrize(
    **utils.gen_bool_params(
        'is_assortment_enrichment_complete', 'enrichment complete',
    ),
)
@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_data_place_categories.sql',
        'fill_force_unavailable_products.sql',
    ],
)
async def test_availability_change_side_effects(
        save_and_update_availability,
        testpoint,
        get_in_progress_assortment,
        sql_mark_assortment_in_progress,
        complete_enrichment_status,
        sql_update_all_places_categories,
        sql_set_need_recalculation,
        sql_has_need_recalculation,
        sql_set_force_unavailable_until,
        sql_get_force_unavailable_until,
        sql_is_force_unavailable,
        update_taxi_config,
        # parametrize params
        change_availability,
        has_active_place_categories,
        has_force_unavailability_just_started,
        has_force_unavailability_just_ended,
        is_assortment_enrichment_complete,
        use_stocks_stq,
):
    update_taxi_config(
        'EATS_NOMENCLATURE_TEMPORARY_CONFIGS',
        {'should_merge_availabilities_in_stocks_stq': use_stocks_stq},
    )

    place_id = 1

    @testpoint('yt-logger-new-availability-old')
    def yt_logger(data):
        del data['timestamp_raw']
        now = dt.datetime.fromisoformat(MOCK_NOW)
        assert data == {
            'place_id': str(place_id),
            'timestamp': now.strftime('%Y-%m-%dT%H:%M:%S'),
        }

    @testpoint('update-category-active-items-for-active')
    def categories_updater_for_active(data):  # pylint: disable=C0103
        pass

    @testpoint('update-category-active-items-for-in-progress')
    def categories_updater_for_in_progress(data):  # pylint: disable=C0103
        pass

    in_progress_assortment_id = get_in_progress_assortment(place_id)
    sql_mark_assortment_in_progress(in_progress_assortment_id)
    complete_enrichment_status(
        place_id,
        {
            'availabilities': False,
            'custom_assortment': is_assortment_enrichment_complete,
        },
    )

    # item 1, 7 have null available_from, 2, 4 have available_from in future
    # item 4, 7 have zero stocks

    new_availabilities = [
        {'origin_id': 'item_origin_1', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_6_force_unavailable', 'available': True},
        {'origin_id': 'item_origin_7_force_unavailable', 'available': False},
    ]
    if change_availability:
        # item_origin_1
        new_availabilities[0]['available'] = True
        # item_origin_2
        new_availabilities[2]['available'] = False

    sql_update_all_places_categories(
        place_id, 1 if has_active_place_categories else 0,
    )

    if has_force_unavailability_just_started:
        sql_set_need_recalculation(place_id, 7, True)
    if has_force_unavailability_just_ended:
        sql_set_need_recalculation(place_id, 6, False)
        sql_set_force_unavailable_until(
            place_id,
            'item_origin_6_force_unavailable',
            OLD_FORCE_UNAVAILABLE_UNTIL,
        )

    await save_and_update_availability(
        new_availabilities, place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )
    end_availability_has_changed = (
        change_availability
        or not has_active_place_categories
        or has_force_unavailability_just_started
        or has_force_unavailability_just_ended
    )
    assert yt_logger.times_called == (
        1 if end_availability_has_changed and not use_stocks_stq else 0
    )

    assert (
        categories_updater_for_active.has_calls == end_availability_has_changed
    )
    assert categories_updater_for_in_progress.has_calls == (
        end_availability_has_changed and is_assortment_enrichment_complete
    )

    assert not sql_has_need_recalculation(place_id, 6)
    assert not sql_has_need_recalculation(place_id, 7)
    if has_force_unavailability_just_ended:
        # force unavailability has just ended only for item_origin_6
        assert not sql_get_force_unavailable_until(
            place_id, 'item_origin_6_force_unavailable',
        )
    else:
        assert sql_is_force_unavailable(
            place_id, 'item_origin_6_force_unavailable',
        )
    assert sql_is_force_unavailable(
        place_id, 'item_origin_7_force_unavailable',
    )


@pytest.mark.parametrize(
    'should_map_origin_to_product_by_brand',
    [
        pytest.param(True, id='map_product_id_by_brand'),
        pytest.param(False, id='map_product_id_by_place'),
    ],
)
@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_data_place_categories.sql',
        'fill_force_unavailable_products.sql',
        'fill_force_unavailable_product_with_wrong_brand.sql',
    ],
)
async def test_recalculation_when_product_brand_changes(
        save_and_update_availability,
        get_in_progress_assortment,
        sql_mark_assortment_in_progress,
        complete_enrichment_status,
        sql_set_need_recalculation,
        sql_has_need_recalculation,
        taxi_config,
        # parametrize params
        should_map_origin_to_product_by_brand,
):
    taxi_config.set_values(
        {
            'EATS_NOMENCLATURE_TEMPORARY_CONFIGS': {
                'should_map_origin_to_product_by_brand': (
                    should_map_origin_to_product_by_brand
                ),
            },
        },
    )

    place_id = 1

    in_progress_assortment_id = get_in_progress_assortment(place_id)
    sql_mark_assortment_in_progress(in_progress_assortment_id)
    complete_enrichment_status(
        place_id, {'availabilities': False, 'custom_assortment': True},
    )

    not_changed_availabilities = [
        {'origin_id': 'item_origin_1', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_6_force_unavailable', 'available': True},
        {'origin_id': 'item_origin_7_force_unavailable', 'available': False},
        {'origin_id': 'item_origin_8_force_unavailable', 'available': False},
    ]

    # set need_recalculation for both product_ids
    sql_set_need_recalculation(
        place_id, 8, True,
    )  # old product_id from previous brand
    sql_set_need_recalculation(place_id, 9, True)  # correct product_id

    await save_and_update_availability(
        not_changed_availabilities,
        place_id=place_id,
        datetime=MOCK_NOW,
        task_id=1,
    )

    if should_map_origin_to_product_by_brand:
        # need_recalculation is reset for correct product_id
        assert sql_has_need_recalculation(place_id, 8)
        assert not sql_has_need_recalculation(place_id, 9)
    else:
        # need_recalculation is reset for wrong product_id
        assert not sql_has_need_recalculation(place_id, 8)
        assert sql_has_need_recalculation(place_id, 9)


@pytest.mark.parametrize(
    'should_include_pennies_in_price',
    [
        pytest.param(False, id='PRICE_ROUNDING config disabled'),
        pytest.param(
            True,
            marks=pytest.mark.config(
                EATS_NOMENCLATURE_PRICE_ROUNDING={
                    '777': {'should_include_pennies_in_price': True},
                },
            ),
            id='PRICE_ROUNDING config with enabled brand',
        ),
    ],
)
@pytest.mark.parametrize(**utils.gen_bool_params('change_availability'))
@pytest.mark.parametrize(
    **utils.gen_bool_params('has_force_unavailability_just_started'),
)
@pytest.mark.parametrize(
    **utils.gen_bool_params('has_force_unavailability_just_ended'),
)
@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_products.sql',
        'fill_force_unavailable_products.sql',
    ],
)
async def test_market_log_place_products(
        testpoint,
        sql_add_place_product,
        save_and_update_availability,
        sql_set_need_recalculation,
        sql_set_force_unavailable_until,
        # parametrize params
        should_include_pennies_in_price,
        change_availability,
        has_force_unavailability_just_started,
        has_force_unavailability_just_ended,
):
    place_id = 1
    wrong_brand_id = 888
    logged_data = []

    @testpoint('place-product-data-producer')
    def logbroker_producer(data):
        decoded_data = base64.b64decode(data)
        export_message = ExportMessage_pb2.ExportMessage()
        export_message.ParseFromString(decoded_data)
        message_dict = json_format.MessageToDict(
            export_message, use_integers_for_enums=False,
        )
        logged_data.append(message_dict)

    # Add product with the same origin_id and place_id but different brand_id.
    available_from = dt.datetime.fromisoformat('2020-02-24 13:00:00+00:00')
    sql_add_place_product(
        'item_origin_5',
        place_id,
        wrong_brand_id,
        available_from=available_from,
    )

    if has_force_unavailability_just_started:
        sql_set_need_recalculation(place_id, 7, True)
    if has_force_unavailability_just_ended:
        sql_set_need_recalculation(place_id, 6, False)
        sql_set_force_unavailable_until(
            place_id,
            'item_origin_6_force_unavailable',
            OLD_FORCE_UNAVAILABLE_UNTIL,
        )

    # item 1, 2, 7 have null available_from
    # item 1, 3, 7 have zero stocks

    new_availabilities = [
        {'origin_id': 'item_origin_1', 'available': False},
        {'origin_id': 'item_origin_2', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_4', 'available': True},
        {'origin_id': 'item_origin_5', 'available': True},
        {'origin_id': 'item_origin_6_force_unavailable', 'available': True},
        {'origin_id': 'item_origin_7_force_unavailable', 'available': False},
    ]
    if change_availability:
        new_availabilities = [
            {'origin_id': 'item_origin_1', 'available': True},
            {'origin_id': 'item_origin_2', 'available': True},
            {'origin_id': 'item_origin_3', 'available': False},
            {'origin_id': 'item_origin_4', 'available': False},
            {'origin_id': 'item_origin_5', 'available': False},
            # don't change availability for force unavailable products
            {
                'origin_id': 'item_origin_6_force_unavailable',
                'available': True,
            },
            {
                'origin_id': 'item_origin_7_force_unavailable',
                'available': False,
            },
        ]

    await save_and_update_availability(
        new_availabilities, place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )
    expected_origin_ids = []
    if change_availability:
        expected_origin_ids += [
            'item_origin_4',
            'item_origin_2',
            'item_origin_5',
        ]
    if has_force_unavailability_just_ended:
        expected_origin_ids += ['item_origin_6_force_unavailable']
    if has_force_unavailability_just_started:
        expected_origin_ids += ['item_origin_7_force_unavailable']

    assert logbroker_producer.has_calls == bool(expected_origin_ids)

    expected_result = get_expected_logged_data(expected_origin_ids)

    if not should_include_pennies_in_price:
        for item in expected_result:
            i_price = item['offer']['shopPrices'][0]['price']
            if i_price['price'] == '4995000000':
                i_price['price'] = '5000000000'
            if 'oldPrice' in i_price and i_price['oldPrice'] == '4995000000':
                i_price['oldPrice'] = '5000000000'

    assert sorted(
        logged_data, key=lambda item: item['offer']['offerId'],
    ) == sorted(expected_result, key=lambda item: item['offer']['offerId'])


@pytest.mark.now(MOCK_NOW)
@pytest.mark.parametrize(
    'brand_ids_black_list',
    [
        pytest.param([], id='BLACK_LIST config disabled'),
        pytest.param([BLACK_LIST_BRAND_ID], id='BLACK_LIST config enabled'),
    ],
)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_products.sql',
        'fill_force_unavailable_products.sql',
        'fill_data_for_brand_778.sql',
    ],
)
async def test_market_log_place_products_black_list(
        testpoint,
        save_and_update_availability,
        update_taxi_config,
        # parametrize params
        brand_ids_black_list,
):
    update_taxi_config(
        'EATS_NOMENCLATURE_EXPORT_TO_MARKET_BRANDS_BLACK_LIST',
        {'brand_ids': brand_ids_black_list},
    )

    logged_data = []

    @testpoint('place-product-data-producer')
    def logbroker_producer(data):
        decoded_data = base64.b64decode(data)
        export_message = ExportMessage_pb2.ExportMessage()
        export_message.ParseFromString(decoded_data)
        message_dict = json_format.MessageToDict(
            export_message, use_integers_for_enums=False,
        )
        logged_data.append(message_dict)

    new_availabilities_1 = [
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': True},
        {'origin_id': 'item_origin_3', 'available': False},
        {'origin_id': 'item_origin_4', 'available': False},
        {'origin_id': 'item_origin_5', 'available': False},
    ]
    await save_and_update_availability(
        new_availabilities_1, place_id=1, datetime=MOCK_NOW, task_id=1,
    )
    expected_origin_ids = ['item_origin_4', 'item_origin_2', 'item_origin_5']
    expected_times_called = len(expected_origin_ids)
    assert logbroker_producer.times_called == expected_times_called

    expected_logged_data = get_expected_logged_data(expected_origin_ids)
    assert (
        sorted(logged_data, key=lambda item: item['offer']['offerId'])
        == sorted(
            expected_logged_data, key=lambda item: item['offer']['offerId'],
        )
    )
    logged_data.clear()
    expected_logged_data.clear()
    expected_origin_ids.clear()

    new_availabilities_2 = [
        {'origin_id': 'item_origin_8_additional', 'available': False},
    ]
    await save_and_update_availability(
        new_availabilities_2, place_id=2, datetime=MOCK_NOW, task_id=2,
    )
    if BLACK_LIST_BRAND_ID not in brand_ids_black_list:
        expected_origin_ids = ['item_origin_8_additional']
        expected_times_called += len(expected_origin_ids)
    assert logbroker_producer.times_called == expected_times_called
    expected_logged_data = get_expected_logged_data(expected_origin_ids)
    assert (
        sorted(logged_data, key=lambda item: item['offer']['offerId'])
        == sorted(
            expected_logged_data, key=lambda item: item['offer']['offerId'],
        )
    )


@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_data.sql',
        'fill_second_place_data.sql',
    ],
)
async def test_category_availability(
        pgsql,
        pg_realdict_cursor,
        get_active_assortment,
        save_and_update_availability,
):
    place_id_1 = 1
    place_id_2 = 2
    active_assortment_id = get_active_assortment(place_id_1)

    def verify_category_availability(
            expected_data,
            place_id=place_id_1,
            assortment_id=active_assortment_id,
    ):
        sql_data = sql_get_place_categories(pgsql, place_id, assortment_id)
        # categories are not removed from db, hence can't use plain `==`
        assert all(
            x in sql_data for x in expected_data
        ), f'expected:\n\t{expected_data}\nrecieved:\n\t{sql_data}'

    async def reset_availability(place_id=1):
        reset_data = [
            {'origin_id': 'item_origin_1', 'available': False},
            {'origin_id': 'item_origin_2', 'available': False},
            {'origin_id': 'item_origin_3', 'available': False},
            {'origin_id': 'item_origin_4', 'available': False},
            {'origin_id': 'item_origin_5', 'available': False},
        ]
        await save_and_update_availability(
            reset_data, place_id=place_id, datetime=MOCK_NOW, task_id=1,
        )
        verify_category_availability(
            [
                {'category_id': 1, 'active_items_count': 0},
                {'category_id': 2, 'active_items_count': 0},
                {'category_id': 3, 'active_items_count': 0},
                {'category_id': 4, 'active_items_count': 0},
                {'category_id': 5, 'active_items_count': 0},
            ],
            place_id,
        )

    # Two available items of category 1
    availability_data = [
        # category 1
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': True},
    ]
    expected_availability_result = [
        {'category_id': 1, 'active_items_count': 1},
    ]

    # post availability data first time
    await save_and_update_availability(
        availability_data, place_id=place_id_1, datetime=MOCK_NOW, task_id=1,
    )
    updated_time_first = sql_get_updated_time(
        pg_realdict_cursor, place_id_1, active_assortment_id,
    )
    updated_time_second = sql_get_updated_time(
        pg_realdict_cursor, place_id_2, active_assortment_id,
    )
    verify_category_availability(expected_availability_result, place_id_1)
    # reset places categories
    sql_update_places_categories(pgsql, place_id_1, active_assortment_id, 0)
    verify_category_availability(
        [
            {'category_id': 1, 'active_items_count': 0},
            {'category_id': 2, 'active_items_count': 0},
            {'category_id': 3, 'active_items_count': 0},
            {'category_id': 4, 'active_items_count': 0},
            {'category_id': 5, 'active_items_count': 0},
        ],
        place_id_1,
    )
    # post the same data, but place categories will be updated
    # because they are null
    await save_and_update_availability(
        availability_data, place_id=place_id_1, datetime=MOCK_NOW, task_id=1,
    )
    verify_category_availability(expected_availability_result, place_id_1)
    updated_last_time_first = sql_get_updated_time(
        pg_realdict_cursor, place_id_1, active_assortment_id,
    )
    updated_last_time_second = sql_get_updated_time(
        pg_realdict_cursor, place_id_2, active_assortment_id,
    )
    assert updated_time_first != updated_last_time_first
    assert updated_time_second == updated_last_time_second
    updated_time_first = updated_last_time_first

    # the same with second place
    availability_data = [
        # category 3
        {'origin_id': 'item_origin_4', 'available': True},
    ]
    expected_availability_result = [
        {'category_id': 3, 'active_items_count': 1},
    ]

    # post availability data first time
    await save_and_update_availability(
        availability_data, place_id=place_id_2, datetime=MOCK_NOW, task_id=1,
    )
    verify_category_availability(expected_availability_result, place_id_2)
    # reset places categories
    sql_update_places_categories(pgsql, place_id_2, active_assortment_id, 0)

    verify_category_availability(
        [
            {'category_id': 1, 'active_items_count': 0},
            {'category_id': 2, 'active_items_count': 0},
            {'category_id': 3, 'active_items_count': 0},
            {'category_id': 4, 'active_items_count': 0},
            {'category_id': 5, 'active_items_count': 0},
        ],
        place_id_2,
    )
    # post the same data, but place categories will be updated
    # because they are null
    await save_and_update_availability(
        availability_data, place_id=place_id_2, datetime=MOCK_NOW, task_id=1,
    )
    verify_category_availability(expected_availability_result, place_id_2)
    updated_last_time_first = sql_get_updated_time(
        pg_realdict_cursor, place_id_1, active_assortment_id,
    )
    updated_last_time_second = sql_get_updated_time(
        pg_realdict_cursor, place_id_2, active_assortment_id,
    )
    assert updated_time_first == updated_last_time_first
    assert updated_time_second != updated_last_time_second
    updated_time_second = updated_last_time_second

    # post the same data, but place categories won't be updated
    # because they aren't null
    await save_and_update_availability(
        availability_data, place_id=place_id_2, datetime=MOCK_NOW, task_id=1,
    )
    updated_last_time_second = sql_get_updated_time(
        pg_realdict_cursor, place_id_2, active_assortment_id,
    )
    assert updated_time_second == updated_last_time_second

    # reset all data because of rule that active_items_count in
    # active categories aren't recalculated
    await reset_availability()

    # One item is not available (false)
    await save_and_update_availability(
        [
            # category 1
            {'origin_id': 'item_origin_1', 'available': False},
            {'origin_id': 'item_origin_2', 'available': True},
        ],
        place_id=place_id_1,
        datetime=MOCK_NOW,
        task_id=1,
    )

    # reset all data as above
    await reset_availability()

    # Items from multiple categories (must have different counts
    # to verify that values are assigned to proper categories)
    await save_and_update_availability(
        [
            # category 1
            {'origin_id': 'item_origin_1', 'available': True},
            {'origin_id': 'item_origin_2', 'available': True},
            # category 3 (child of category 2)
            {'origin_id': 'item_origin_3', 'available': True},
        ],
        place_id=place_id_1,
        datetime=MOCK_NOW,
        task_id=1,
    )
    verify_category_availability(
        [
            {'category_id': 1, 'active_items_count': 1},
            {'category_id': 2, 'active_items_count': 1},
            {'category_id': 3, 'active_items_count': 1},
            {'category_id': 4, 'active_items_count': 0},
            {'category_id': 5, 'active_items_count': 0},
        ],
    )

    # Verify that stock values are correctly taken in account
    # and not overwritten.
    # Note: category availability is only recalculated if there
    # are actual changes in product availability, thus we have
    # to clear data first.
    sql_set_stock_value(
        pgsql, place_id_1, active_assortment_id, 'item_origin_1', 0,
    )
    await save_and_update_availability(
        [
            # category 1
            {'available': False, 'origin_id': 'item_origin_1'},
            {'available': False, 'origin_id': 'item_origin_2'},
            # category 3 (child of category 2)
            {'available': False, 'origin_id': 'item_origin_3'},
        ],
        place_id=place_id_1,
        datetime=MOCK_NOW,
        task_id=1,
    )
    await save_and_update_availability(
        [
            # category 1
            {'available': True, 'origin_id': 'item_origin_1'},
            {'available': True, 'origin_id': 'item_origin_2'},
            # category 3 (child of category 2)
            {'available': True, 'origin_id': 'item_origin_3'},
        ],
        place_id=place_id_1,
        datetime=MOCK_NOW,
        task_id=1,
    )
    verify_category_availability(
        [
            {'category_id': 1, 'active_items_count': 1},
            {'category_id': 2, 'active_items_count': 1},
        ],
    )


@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data_two_categories.sql'],
)
async def test_category_availability_thresholds(
        pgsql, save_and_update_availability, get_active_assortment,
):
    place_id = 1
    active_assortment_id = get_active_assortment(place_id)

    def verify_category_availability(
            expected_data,
            place_id=place_id,
            assortment_id=active_assortment_id,
    ):
        sql_data = sql_get_place_categories(pgsql, place_id, assortment_id)
        # categories are not removed from db, hence can't use plain `==`
        assert all(
            x in sql_data for x in expected_data
        ), f'expected:\n\t{expected_data}\nrecieved:\n\t{sql_data}'

    verify_category_availability(
        [
            {'category_id': 1, 'active_items_count': 0},
            {'category_id': 2, 'active_items_count': 0},
        ],
    )

    # check 0->2, 0->1 active items
    await save_and_update_availability(
        [
            {'origin_id': 'item_origin_1', 'available': True},
            {'origin_id': 'item_origin_2', 'available': True},
            {'origin_id': 'item_origin_4', 'available': True},
        ],
        place_id=1,
        datetime=MOCK_NOW,
        task_id=1,
    )

    expected_categories = [
        {'category_id': 1, 'active_items_count': 1},
        {'category_id': 2, 'active_items_count': 1},
    ]
    verify_category_availability(expected_categories)

    # check 2->3, 0->2 active items
    await save_and_update_availability(
        [
            {'origin_id': 'item_origin_1', 'available': True},
            {'origin_id': 'item_origin_2', 'available': True},
            {'origin_id': 'item_origin_3', 'available': True},
            {'origin_id': 'item_origin_4', 'available': True},
            {'origin_id': 'item_origin_5', 'available': True},
        ],
        place_id=1,
        datetime=MOCK_NOW,
        task_id=1,
    )
    expected_categories = [
        {'category_id': 1, 'active_items_count': 1},
        {'category_id': 2, 'active_items_count': 1},
    ]
    verify_category_availability(expected_categories)

    # check 3->2, 2->0 active items
    await save_and_update_availability(
        [
            {'origin_id': 'item_origin_1', 'available': True},
            {'origin_id': 'item_origin_2', 'available': True},
            {'origin_id': 'item_origin_3', 'available': False},
            {'origin_id': 'item_origin_4', 'available': False},
            {'origin_id': 'item_origin_5', 'available': False},
        ],
        place_id=1,
        datetime=MOCK_NOW,
        task_id=1,
    )
    expected_categories = [
        {'category_id': 1, 'active_items_count': 1},
        {'category_id': 2, 'active_items_count': 0},
    ]
    verify_category_availability(expected_categories)


@pytest.mark.now(MOCK_NOW)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=[
        'fill_dictionaries.sql',
        'fill_place_data_two_categories.sql',
        'fill_force_unavailable_products.sql',
    ],
)
async def test_category_availability_with_force_unavailable(
        pgsql,
        save_and_update_availability,
        get_active_assortment,
        sql_set_need_recalculation,
):
    place_id = 1
    active_assortment_id = get_active_assortment(place_id)

    def verify_category_availability(
            expected_data,
            place_id=place_id,
            assortment_id=active_assortment_id,
    ):
        sql_data = sql_get_place_categories(pgsql, place_id, assortment_id)
        # categories are not removed from db, hence can't use plain `==`
        assert all(
            x in sql_data for x in expected_data
        ), f'expected:\n\t{expected_data}\nrecieved:\n\t{sql_data}'

    verify_category_availability(
        [{'category_id': 100, 'active_items_count': 2}],
    )

    current_availability = [
        {'origin_id': 'item_origin_6_force_unavailable', 'available': True},
        {'origin_id': 'item_origin_7_force_unavailable', 'available': False},
    ]

    sql_set_need_recalculation(place_id, 6, False)
    sql_set_need_recalculation(place_id, 7, False)

    # send same availability as current
    await save_and_update_availability(
        current_availability, place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )

    # availabilities are not changed and need_recalculation is false
    # so categories are not recalculated
    verify_category_availability(
        [{'category_id': 100, 'active_items_count': 2}],
    )

    sql_set_need_recalculation(place_id, 6, True)
    sql_set_need_recalculation(place_id, 7, True)

    # send same availability as current
    await save_and_update_availability(
        current_availability, place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )

    # need_recalculation is true - categories are recalculated
    verify_category_availability(
        [{'category_id': 100, 'active_items_count': 0}],
    )


@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_availability_throttling(
        taxi_config, pgsql, save_and_update_availability,
):
    taxi_config.set_values(metrics_config('update_availability', -1))

    old_availabilities = sql_get_availabilities(pgsql)

    availabilities_data = get_availability_with_changes()

    await save_and_update_availability(
        availabilities_data, place_id=1, datetime=MOCK_NOW, task_id=1,
    )
    assert sql_get_availabilities(pgsql) == old_availabilities


@pytest.mark.config(
    EATS_NOMENCLATURE_STOCKS_AND_AVAILABILITY_UPDATE_SETTINGS={
        'maximum_update_interval_in_min': 5,
    },
)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_availability_partly_throttling(
        taxi_config, pgsql, save_and_update_availability,
):
    taxi_config.set_values(metrics_config('update_availability', -1))

    old_availabilities = sql_get_availabilities(pgsql)

    sql_set_place_update_statuses(pgsql, interval_minutes=2)

    availabilities_data = get_availability_with_changes()
    await save_and_update_availability(
        availabilities_data, place_id=1, datetime=MOCK_NOW, task_id=1,
    )
    assert sql_get_availabilities(pgsql) == old_availabilities

    sql_upd_place_update_statuses(pgsql, interval_minutes=10)

    await save_and_update_availability(
        availabilities_data, place_id=1, datetime=MOCK_NOW, task_id=1,
    )
    assert sql_get_availabilities(pgsql) != old_availabilities


@pytest.mark.now(MOCK_NOW)
@pytest.mark.parametrize(
    **utils.gen_bool_params(
        'is_assortment_enrichment_complete', 'enrichment complete',
    ),
)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_finish_processing(
        testpoint,
        stq,
        complete_enrichment_status,
        save_and_update_availability,
        sql_mark_assortment_in_progress,
        renew_in_progress_assortment,
        duplicate_assortment_data,
        sql_are_availabilities_ready,
        # parametrize params
        is_assortment_enrichment_complete,
):
    place_id = 1

    in_progress_assortment_id = renew_in_progress_assortment(place_id)
    sql_mark_assortment_in_progress(in_progress_assortment_id)
    complete_enrichment_status(
        place_id,
        {
            'availabilities': False,
            'custom_assortment': is_assortment_enrichment_complete,
        },
    )

    duplicate_assortment_data(in_progress_assortment_id, 1)

    await save_and_update_availability(
        [], place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )

    # availabilities should be marked ready
    # only after assortment enrichment is complete
    assert (
        sql_are_availabilities_ready(place_id)
        == is_assortment_enrichment_complete
    )

    assert (
        stq.eats_nomenclature_assortment_activation_notifier.has_calls
        == is_assortment_enrichment_complete
    )


@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_enqueue_fts_indexer_stq(
        stq,
        stq_call_forward,
        complete_enrichment_status,
        sql_mark_assortment_in_progress,
        renew_in_progress_assortment,
        duplicate_assortment_data,
        save_and_update_availability,
):
    place_id = 1
    place_slug = '1'
    in_progress_assortment_id = renew_in_progress_assortment(place_id)
    sql_mark_assortment_in_progress(in_progress_assortment_id)
    complete_enrichment_status(
        place_id, {'availabilities': False, 'custom_assortment': True},
    )

    duplicate_assortment_data(in_progress_assortment_id, 1)

    availabilities_data = get_availability_with_changes()
    await save_and_update_availability(
        availabilities_data, place_id=place_id, datetime=MOCK_NOW, task_id=1,
    )

    stq_next_call = (
        stq.eats_nomenclature_assortment_activation_notifier.next_call()
    )
    assert stq_next_call['kwargs']['place_ids'] == [place_id]
    await stq_call_forward(stq_next_call)

    task_info = (
        stq.eats_full_text_search_indexer_update_retail_place.next_call()
    )
    assert task_info['id'] == place_slug
    assert task_info['kwargs']['place_slug'] == place_slug


@pytest.mark.config(
    EATS_NOMENCLATURE_PROCESSING=settings(max_retries_on_error=2),
)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_stq_error_limit(
        task_enqueue_v2, taxi_config, put_availability_data_to_s3,
):
    max_retries_on_error = taxi_config.get('EATS_NOMENCLATURE_PROCESSING')[
        '__default__'
    ]['max_retries_on_error']
    task_id = '1'
    place_id = '1'
    kwargs = {
        'place_id': place_id,
        's3_path': S3_AVAILABILITY_PATH,
        'file_datetime': TEST_DATETIME,
    }
    invalid_availabilities_data = 1
    await put_availability_data_to_s3(invalid_availabilities_data)

    for i in range(max_retries_on_error):
        await task_enqueue_v2(
            QUEUE_NAME,
            task_id=task_id,
            kwargs=kwargs,
            expect_fail=True,
            exec_tries=i,
        )

    # should succeed because of the error limit
    await task_enqueue_v2(
        QUEUE_NAME,
        task_id=task_id,
        kwargs=kwargs,
        expect_fail=False,
        exec_tries=max_retries_on_error,
    )


@pytest.mark.config(
    EATS_NOMENCLATURE_PROCESSING=settings(
        max_retries_on_busy=2, max_busy_time_in_ms=100000,
    ),
)
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_stq_busy_limit(
        mockserver,
        task_enqueue_v2,
        taxi_config,
        put_availability_data_to_s3,
        sql_set_place_busy,
        sql_is_place_busy,
):
    config = taxi_config.get('EATS_NOMENCLATURE_PROCESSING')['__default__']
    max_retries_on_busy = config['max_retries_on_busy']
    max_busy_time_in_ms = config['max_busy_time_in_ms']
    retry_on_busy_delay_ms = config['retry_on_busy_delay_ms']
    task_id = '1'
    place_id = '1'
    kwargs = {
        'place_id': place_id,
        's3_path': S3_AVAILABILITY_PATH,
        'file_datetime': TEST_DATETIME,
    }
    availabilities_data = get_availability_with_changes()
    await put_availability_data_to_s3(availabilities_data)

    @mockserver.json_handler('/stq-agent/queues/api/reschedule')
    async def mock_stq_reschedule(request):
        data = request.json
        assert data['queue_name'] == QUEUE_NAME
        assert data['task_id'] == task_id

        eta = du.parser.parse(data['eta']).replace(tzinfo=None)
        assert (
            eta - dt.datetime.now()
        ).total_seconds() < retry_on_busy_delay_ms

        return {}

    # initialize data
    await task_enqueue_v2(QUEUE_NAME, task_id=task_id, kwargs=kwargs)

    sql_set_place_busy(PLACE_UPDATE_STATUS_PREFIX, place_id)

    assert mock_stq_reschedule.times_called == 0

    for i in range(max_retries_on_busy):
        await task_enqueue_v2(
            QUEUE_NAME, task_id=task_id, kwargs=kwargs, reschedule_counter=i,
        )
        assert mock_stq_reschedule.times_called == i + 1
        assert sql_is_place_busy(PLACE_UPDATE_STATUS_PREFIX, place_id)

    # Exceed max busy retries
    await task_enqueue_v2(
        QUEUE_NAME,
        task_id=task_id,
        kwargs=kwargs,
        reschedule_counter=max_retries_on_busy,
    )
    assert mock_stq_reschedule.times_called == max_retries_on_busy
    assert sql_is_place_busy(PLACE_UPDATE_STATUS_PREFIX, place_id)

    # Expire the busy status
    sql_set_place_busy(
        PLACE_UPDATE_STATUS_PREFIX, place_id, 3 * max_busy_time_in_ms,
    )
    await task_enqueue_v2(
        QUEUE_NAME, task_id=task_id, kwargs=kwargs, reschedule_counter=0,
    )
    assert mock_stq_reschedule.times_called == max_retries_on_busy
    assert not sql_is_place_busy(PLACE_UPDATE_STATUS_PREFIX, place_id)


@pytest.mark.parametrize(**utils.gen_bool_params('merge_data_in_stq'))
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_task_with_not_found_file(
        pgsql,
        update_taxi_config,
        availability_enqueue,
        # parametrize
        merge_data_in_stq,
):
    update_taxi_config(
        'EATS_NOMENCLATURE_TEMPORARY_CONFIGS',
        {'should_merge_data_in_availability_stq': merge_data_in_stq},
    )

    place_id = 1
    wrong_path = 'wrong_path'

    await availability_enqueue(place_id, s3_path=wrong_path)

    assert sql_get_errors(pgsql, place_id=place_id) == (
        'failed',
        'Availability file is not found',
        'Availability file is not found at path ' + wrong_path,
    )


@pytest.mark.parametrize(**utils.gen_bool_params('merge_data_in_stq'))
@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_processed_task_in_s3(
        put_availability_data_to_s3,
        assert_availability_file_in_s3_and_db,
        availability_enqueue,
        update_taxi_config,
        # parametrize
        merge_data_in_stq,
):
    update_taxi_config(
        'EATS_NOMENCLATURE_TEMPORARY_CONFIGS',
        {'should_merge_data_in_availability_stq': merge_data_in_stq},
    )

    place_id = 1
    new_availabilities = [
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_5', 'available': False},
    ]

    await put_availability_data_to_s3(
        new_availabilities, S3_AVAILABILITY_PATH, place_id,
    )
    await availability_enqueue(
        place_id, S3_AVAILABILITY_PATH, TEST_DATETIME, str(place_id),
    )
    assert_availability_file_in_s3_and_db(
        place_id, S3_OLD_AVAILABILITY_PATH, TEST_DATETIME, new_availabilities,
    )


@pytest.mark.pgsql(
    'eats_nomenclature',
    files=['fill_dictionaries.sql', 'fill_place_data.sql'],
)
async def test_disabled_merge(
        pgsql,
        put_availability_data_to_s3,
        availability_enqueue,
        update_taxi_config,
):
    update_taxi_config(
        'EATS_NOMENCLATURE_TEMPORARY_CONFIGS',
        {'should_merge_data_in_availability_stq': False},
    )

    place_id = 1
    new_availabilities = [
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
        {'origin_id': 'item_origin_5', 'available': False},
    ]
    old_availabilities = sql_get_availabilities(pgsql)

    await put_availability_data_to_s3(
        new_availabilities, S3_AVAILABILITY_PATH, place_id,
    )
    await availability_enqueue(
        place_id, S3_AVAILABILITY_PATH, TEST_DATETIME, str(place_id),
    )
    # data should not be changed, since merge is disabled
    assert sql_get_availabilities(pgsql) == old_availabilities


def get_availability_with_changes():
    return [
        {'origin_id': 'item_origin_1', 'available': True},
        {'origin_id': 'item_origin_2', 'available': False},
        {'origin_id': 'item_origin_3', 'available': True},
    ]


def sql_get_errors(pgsql, place_id):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        f"""
        select status, task_error, task_error_details
        from eats_nomenclature.places_processing_last_status_v2
        where place_id = {place_id}
        """,
    )
    return cursor.fetchone()


def sql_get_availabilities(pgsql):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        """
        select p.origin_id, pp.available_from, p.brand_id
        from eats_nomenclature.places_products pp
          join eats_nomenclature.products p on pp.product_id = p.id
        """,
    )
    # Removing microseconds,
    # since API provides epoch time with precision in seconds
    return {
        (i[0], i[1].replace(microsecond=0) if i[1] else None, i[2])
        for i in cursor
    }


def metrics_config(name, max_dead_tuples_):
    return {
        'EATS_NOMENCLATURE_METRICS': {
            '__default__': {
                'assortment_outdated_threshold_in_hours': 2,
                'max_dead_tuples': 1000000,
            },
            name: {
                'assortment_outdated_threshold_in_hours': 2,
                'max_dead_tuples': max_dead_tuples_,
            },
        },
    }


def sql_get_place_categories(pgsql, place_id, assortment_id):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        f"""
        select category_id, active_items_count
        from eats_nomenclature.places_categories
        where place_id = {place_id} and assortment_id = {assortment_id}
        """,
    )
    return [{'category_id': i[0], 'active_items_count': i[1]} for i in cursor]


def sql_get_updated_time(pg_realdict_cursor, place_id, assortment_id):
    pg_realdict_cursor.execute(
        """
        select category_id, updated_at
        from eats_nomenclature.places_categories
        where place_id = %s and assortment_id = %s
        """,
        (place_id, assortment_id),
    )
    return pg_realdict_cursor.fetchall()


def sql_update_places_categories(pgsql, place_id, assortment_id, value):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        f"""
        update eats_nomenclature.places_categories
        set active_items_count = {value}
        where place_id = {place_id} and assortment_id = {assortment_id}""",
    )


def sql_set_stock_value(pgsql, place_id, assortment_id, origin_id, value):
    cursor = pgsql['eats_nomenclature'].cursor()

    cursor.execute(
        """
        update eats_nomenclature.stocks
        set value = %s
        where
          place_product_id in (
            select id
            from eats_nomenclature.categories_products cp
              join eats_nomenclature.places_products pp
                on pp.product_id = cp.product_id
            where
              pp.place_id = %s
              and cp.assortment_id = %s
              and pp.origin_id = %s
          )
        """,
        (value, place_id, assortment_id, origin_id),
    )


def sql_set_place_update_statuses(pgsql, interval_minutes):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        f"""
        insert into eats_nomenclature.place_update_statuses
        (place_id, availability_update_started_at)
        values (1, now() - interval '{interval_minutes} minutes');""",
    )


def sql_upd_place_update_statuses(pgsql, interval_minutes):
    cursor = pgsql['eats_nomenclature'].cursor()
    cursor.execute(
        f"""
        update eats_nomenclature.place_update_statuses
        set availability_update_started_at =
            now() - interval '{interval_minutes} minutes',
            updated_at = now()
        where place_id = 1;""",
    )


def get_expected_logged_data(expected_origin_ids):
    all_offers = [
        {
            'offer': {
                'businessId': 40,
                'offerId': '00000000-0000-0000-0000-000000000008',
                'shopId': 50,
                'feedId': 60,
                'originalSku': 'item_origin_8_additional',
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 50,
                        'price': {
                            'currency': 'RUR',
                            'price': '2500000000',
                            'oldPrice': '2000000000',
                        },
                        'vat': 'VAT_0',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 50,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': True,
                            },
                        },
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'offerId': '22222222-2222-2222-2222-222222222222',
                'originalSku': 'item_origin_2',
                'shopId': 20,
                'feedId': 30,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 20,
                        'price': {
                            'currency': 'RUR',
                            'price': '9990000000',
                            'oldPrice': '100000000',
                        },
                        'vat': 'VAT_10',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 20,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': False,
                            },
                        },
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'feedId': 30,
                'offerId': '33333333-3333-3333-3333-333333333333',
                'originalSku': 'item_origin_3',
                'shopId': 20,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'price': {
                            'currency': 'RUR',
                            'oldPrice': '1500000000',
                            'price': '2000000000',
                        },
                        'shopId': 20,
                        'vat': 'VAT_0',
                    },
                ],
                'shopStatuses': [
                    {
                        'disableStatus': {
                            3: {
                                'flag': False,
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                            },
                        },
                        'shopId': 20,
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'offerId': '44444444-4444-4444-4444-444444444444',
                'originalSku': 'item_origin_4',
                'shopId': 20,
                'feedId': 30,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 20,
                        'price': {'currency': 'RUR', 'price': '0'},
                        'vat': 'WRONG_VAT',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 20,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': True,
                            },
                        },
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'offerId': '55555555-5555-5555-5555-555555555555',
                'originalSku': 'item_origin_5',
                'shopId': 20,
                'feedId': 30,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 20,
                        'price': {
                            'currency': 'RUR',
                            'price': '2500000000',
                            'oldPrice': '2000000000',
                        },
                        'vat': 'VAT_0',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 20,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': True,
                            },
                        },
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'offerId': '66666666-6666-6666-6666-666666666666',
                'originalSku': 'item_origin_6_force_unavailable',
                'shopId': 20,
                'feedId': 30,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 20,
                        'price': {
                            'currency': 'RUR',
                            'price': '4995000000',
                            'oldPrice': '4995000000',
                        },
                        'vat': 'WRONG_VAT',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 20,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': False,
                            },
                        },
                    },
                ],
            },
        },
        {
            'offer': {
                'businessId': 10,
                'offerId': '77777777-7777-7777-7777-777777777777',
                'originalSku': 'item_origin_7_force_unavailable',
                'shopId': 20,
                'feedId': 30,
                'shopPrices': [
                    {
                        'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                        'shopId': 20,
                        'price': {
                            'currency': 'RUR',
                            'price': '4995000000',
                            'oldPrice': '4995000000',
                        },
                        'vat': 'WRONG_VAT',
                    },
                ],
                'shopStatuses': [
                    {
                        'shopId': 20,
                        'disableStatus': {
                            3: {
                                'meta': {'timestamp': '2020-02-24T13:00:00Z'},
                                'flag': True,
                            },
                        },
                    },
                ],
            },
        },
    ]

    return list(
        filter(
            lambda i: (i['offer']['originalSku'] in expected_origin_ids),
            all_offers,
        ),
    )


@pytest.fixture(name='assert_availability_file_in_s3_and_db')
def _assert_availability_file_in_s3_and_db(assert_entity_file_in_s3_and_db):
    def impl(
            place_id,
            expected_file_path=None,
            expected_file_datetime=None,
            expected_file_availabilities=None,
            expect_no_file=False,
    ):
        assert_entity_file_in_s3_and_db(
            'availability_files',
            place_id=place_id,
            expected_file_path=expected_file_path,
            expected_file_datetime=expected_file_datetime,
            expected_file_items=expected_file_availabilities,
            expect_no_file=expect_no_file,
        )

    return impl


@pytest.fixture(name='save_and_update_availability')
def _save_and_update_availability(
        put_availability_data_to_s3, availability_enqueue,
):
    async def impl(data, place_id, datetime=MOCK_NOW, task_id=1):
        await put_availability_data_to_s3(data, S3_AVAILABILITY_PATH, place_id)
        await availability_enqueue(
            place_id, S3_AVAILABILITY_PATH, datetime, str(task_id),
        )

    return impl
