{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:20.092771Z",
     "start_time": "2019-05-30T13:11:20.087466Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:21.313526Z",
     "start_time": "2019-05-30T13:11:20.957079Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from nile.api.v1 import Record\n",
    "from nile.api.v1 import filters as nf\n",
    "from nile.api.v1 import extractors as ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:22.140693Z",
     "start_time": "2019-05-30T13:11:22.135334Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:23.074602Z",
     "start_time": "2019-05-30T13:11:22.794442Z"
    }
   },
   "outputs": [],
   "source": [
    "from zoo.remote_quality_control.dashboard import project_config as config\n",
    "from pymlaas.models.remote_quality_control import constants\n",
    "from zoo.utils.nile_helpers import dates\n",
    "\n",
    "reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:26.422342Z",
     "start_time": "2019-05-30T13:11:23.789915Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = config.get_project_cluster()\n",
    "job = cluster.job(). env(parallel_operations_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:26.517217Z",
     "start_time": "2019-05-30T13:11:26.513039Z"
    }
   },
   "outputs": [],
   "source": [
    "START_DATE = '2018-12-01'\n",
    "FINISH_DATE = '2019-05-29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:26.649930Z",
     "start_time": "2019-05-30T13:11:26.604087Z"
    }
   },
   "outputs": [],
   "source": [
    "def exam_group_reducer(groups):\n",
    "\n",
    "    for exam_id, records in groups:\n",
    "\n",
    "        qc_type_list = []\n",
    "        assessor_type_list = []\n",
    "        resolution_list = []\n",
    "        assessor_comment_list = []\n",
    "        ml_list = []\n",
    "        country_list = []\n",
    "\n",
    "        final_resolution = None\n",
    "        final_resolver = None\n",
    "\n",
    "        for record in records:\n",
    "            qc_type_list.append(record.get('qc_type'))\n",
    "            assessor_type_list.append(record.get('assessor_type'))\n",
    "            resolution_list.append(record.get('resolution'))\n",
    "            assessor_comment_list.append(record.get('assessor_comment'))\n",
    "            ml_list.append(record.get('ml'))\n",
    "            country_list.append(record.get('country'))\n",
    "            \n",
    "            if record.get('resolution') == u'success':\n",
    "                final_resolution = u'success'\n",
    "                final_resolver = record.get('assessor')\n",
    "            elif record.get('resolution') == u'block':\n",
    "                final_resolution = u'block'\n",
    "\n",
    "        if (config.RESOLUTION_SET.intersection(set(resolution_list))\n",
    "                and u'human' in assessor_type_list):\n",
    "            final_resolver = u'assessor'\n",
    "        elif (config.RESOLUTION_SET.intersection(set(resolution_list))\n",
    "                and set(assessor_type_list) == set([u'ml', None])):\n",
    "            final_resolver = u'ml'\n",
    "        elif (config.RESOLUTION_SET.intersection(set(resolution_list))\n",
    "                and set(assessor_type_list) != set([u'ml'])\n",
    "                and u'human' not in assessor_type_list):\n",
    "            final_resolver = u'toloka'\n",
    "\n",
    "        \n",
    "        country = filter(None, country_list)\n",
    "        if country:\n",
    "            country = country[0]\n",
    "        else:\n",
    "            country = None\n",
    "        city = record.get('city').decode('utf-8').lower()\n",
    "        car_brand = record.get('car_brand')\n",
    "        car_model = record.get('car_model')\n",
    "        car_color = record.get('car_color')\n",
    "        car_number = record.get('car_number')\n",
    "        car_year = record.get('car_year')\n",
    "        qc_date = record.get('qc_date')\n",
    "\n",
    "        yield Record(exam_id=exam_id,\n",
    "                     qc_type_list=qc_type_list,\n",
    "                     assessor_type_list=assessor_type_list,\n",
    "                     resolution_list=resolution_list,\n",
    "                     assessor_comment_list=assessor_comment_list,\n",
    "                     ml_list=ml_list,\n",
    "                     city=city,\n",
    "                     car_brand=car_brand,\n",
    "                     car_model=car_model,\n",
    "                     car_color=car_color,\n",
    "                     car_number=car_number,\n",
    "                     car_year=car_year,\n",
    "                     qc_date=qc_date,\n",
    "                     final_resolution=final_resolution,\n",
    "                     final_resolver=final_resolver,\n",
    "                     country=country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:27.324202Z",
     "start_time": "2019-05-30T13:11:26.740137Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_exam_group_mapper(date_to_map):\n",
    "\n",
    "    def exam_group_mapper(records):\n",
    "\n",
    "        counts = {'fielddate': date_to_map}\n",
    "\n",
    "        # Overall metrics.\n",
    "        counts['total'] = 0\n",
    "        counts['total_int'] = 0\n",
    "        counts['total_internat'] = 0\n",
    "        counts['total_ml'] = 0\n",
    "        counts['total_ml_int'] = 0\n",
    "        counts['total_ml_internat'] = 0\n",
    "\n",
    "        counts['no_ml'] = 0\n",
    "        counts['no_ml_int'] = 0\n",
    "        counts['no_ml_internat'] = 0\n",
    "\n",
    "        counts['use_ml_false'] = 0\n",
    "        counts['use_ml_false_int'] = 0\n",
    "        counts['use_ml_false_ext'] = 0\n",
    "        counts['use_ml_false_internat'] = 0\n",
    "\n",
    "        counts['total_no_full_ml_resp'] = 0\n",
    "\n",
    "        counts['total_exp'] = 0\n",
    "        counts['total_exp_ml_unknown'] = 0\n",
    "        counts['total_exp_asr_unknown'] = 0\n",
    "        counts['total_exp_int'] = 0\n",
    "        counts['total_exp_internat'] = 0\n",
    "\n",
    "        counts['success_ml_prod'] = 0\n",
    "        counts['block_ml_prod'] = 0\n",
    "        counts['unknown_ml_prod'] = 0\n",
    "\n",
    "        counts['success_ml_back'] = 0\n",
    "        counts['block_ml_back'] = 0\n",
    "        counts['unknown_ml_back'] = 0\n",
    "\n",
    "        counts['success_ml_ext'] = 0\n",
    "        counts['block_ml_ext'] = 0\n",
    "        counts['unknown_ml_ext'] = 0\n",
    "\n",
    "        counts['success_ml_int'] = 0\n",
    "        counts['block_ml_int'] = 0\n",
    "        counts['unknown_ml_int'] = 0\n",
    "\n",
    "        counts['success_ml_internat'] = 0\n",
    "        counts['block_ml_internat'] = 0\n",
    "        counts['unknown_ml_internat'] = 0\n",
    "\n",
    "        # Reasons of blocks.\n",
    "        for background_block_reason in config.BACKGROUND_BLOCK_REASON_SET:\n",
    "            counts[u'back_' + background_block_reason.lower()] = 0\n",
    "        for background_block_reason in config.BACKGROUND_BLOCK_REASON_SET:\n",
    "            counts[u'back_' + background_block_reason.lower() + '_internat'] = 0\n",
    "\n",
    "        for prod_reason in config.PROD_BLOCK_REASON_SET:\n",
    "            counts[u'prod_' + prod_reason.lower()] = 0\n",
    "        for prod_reason in config.PROD_BLOCK_REASON_SET:\n",
    "            counts[u'prod_' + prod_reason.lower() + '_internat'] = 0\n",
    "\n",
    "        # Experiment metrics.\n",
    "        for experiment_metric in config.EXPERIMENT_METRICS:\n",
    "            counts[experiment_metric] = 0\n",
    "\n",
    "        counts['success_assessor_exp_ml'] = 0\n",
    "        counts['block_assessor_exp_ml'] = 0\n",
    "        counts['success_assessor_exp_noml'] = 0\n",
    "        counts['block_assessor_exp_noml'] = 0\n",
    "\n",
    "        # Ranking metrics.\n",
    "        ranking_score_list = []\n",
    "        ranking_decision_list = []\n",
    "        for ranking_drop_prop in config.RANKING_DROP_PROPORTIONS:\n",
    "            counts['success_bottom_' + str(ranking_drop_prop) + '_ml'] = 0\n",
    "            counts['block_bottom_' + str(ranking_drop_prop) + '_ml'] = 0\n",
    "            counts['success_bottom_' + str(ranking_drop_prop) + '_random'] = 0\n",
    "            counts['block_bottom_' + str(ranking_drop_prop) + '_random'] = 0\n",
    "        \n",
    "        for rqc_type in config.RELEVANT_RQC_TYPES:\n",
    "            counts[rqc_type] = 0\n",
    "        \n",
    "        for record in records:\n",
    "            # Flags.\n",
    "            passed_through_ml = False\n",
    "            international_exam = (record.get('country')\n",
    "                                  not in {'Россия', u'Россия'})\n",
    "            no_ml = True\n",
    "            use_ml_true = False\n",
    "            interior_exam_enabled = (record.get('city').lower()\n",
    "                                     in config.INTERIOR_CITIES)\n",
    "            full_ml_response = False\n",
    "\n",
    "            counts['total'] += 1\n",
    "            if interior_exam_enabled:\n",
    "                counts['total_int'] += 1\n",
    "            if international_exam:\n",
    "                counts['total_internat'] += 1\n",
    "            \n",
    "            for rqc_type in config.RELEVANT_RQC_TYPES:\n",
    "                if rqc_type in record['qc_type_list']:\n",
    "                    counts[rqc_type] += 1\n",
    "            \n",
    "            if not filter(None, record.get('ml_list')):\n",
    "                ml_response = {}\n",
    "                counts['no_ml'] += 1\n",
    "                if interior_exam_enabled:\n",
    "                    counts['no_ml_int'] += 1\n",
    "                if international_exam:\n",
    "                    counts['no_ml_internat'] += 1\n",
    "            else:\n",
    "                ml_response = filter(None, record.get('ml_list'))[0]\n",
    "                \n",
    "                full_ml_response = all(\n",
    "                    [field in ml_response for field in config.REQUIRED_FIELDS]\n",
    "                )\n",
    "                if not full_ml_response:\n",
    "                    counts['total_no_full_ml_resp'] += 1\n",
    "                    continue\n",
    "                \n",
    "                use_ml_true = (\n",
    "                    ml_response.get('result_mismatch_reason')\n",
    "                    != u'use_ml_false'\n",
    "                )\n",
    "                if not use_ml_true:\n",
    "                    counts['use_ml_false'] += 1\n",
    "                    counts['use_ml_false_ext'] += 1\n",
    "                    if international_exam:\n",
    "                        counts['use_ml_false_internat'] += 1\n",
    "                    if interior_exam_enabled:\n",
    "                        counts['use_ml_false_int'] += 1\n",
    "\n",
    "                passed_through_ml = True\n",
    "                counts['total_ml'] += 1\n",
    "                if interior_exam_enabled:\n",
    "                    counts['total_ml_int'] += 1\n",
    "                if international_exam:\n",
    "                    counts['total_ml_internat'] += 1\n",
    "                \n",
    "                experiment = (u'experiment_double_check'\n",
    "                              in ml_response.get('experiments_list', {}))\n",
    "                passed_to_assessor = (ml_response.get('exam_inspector')\n",
    "                                      == u'assessor')\n",
    "                if experiment:\n",
    "                    counts['total_exp'] += 1\n",
    "                    if interior_exam_enabled:\n",
    "                        counts['total_exp_int'] += 1\n",
    "                    if international_exam:\n",
    "                        counts['total_exp_internat'] += 1\n",
    "\n",
    "            if ml_response:\n",
    "\n",
    "                # Overall.\n",
    "                if use_ml_true and not passed_to_assessor:\n",
    "                    if (record.get('final_resolution') == u'success'\n",
    "                            and ml_response['result'] == u'success'):\n",
    "                        if u'exam_inspector' in ml_response:\n",
    "                            if ml_response.get('exam_inspector') == u'ml':\n",
    "                                counts['success_ml_prod'] += 1\n",
    "                                if international_exam:\n",
    "                                    counts['success_ml_internat'] += 1\n",
    "                        else:\n",
    "                            counts['success_ml_prod'] += 1\n",
    "                            if international_exam:\n",
    "                                counts['success_ml_internat'] += 1\n",
    "                    elif (record.get('final_resolution') == u'block'\n",
    "                            and ml_response['result'] == u'block'):\n",
    "                        if u'exam_inspector' in ml_response:\n",
    "                            if ml_response.get('exam_inspector') == u'ml':\n",
    "                                counts['block_ml_prod'] += 1\n",
    "                                if international_exam:\n",
    "                                    counts['block_ml_internat'] += 1\n",
    "                        else:\n",
    "                            counts['block_ml_prod'] += 1\n",
    "                            if international_exam:\n",
    "                                counts['block_ml_internat'] += 1\n",
    "                    elif ml_response['result'] == u'unknown':\n",
    "                        counts['unknown_ml_prod'] += 1\n",
    "                        if international_exam:\n",
    "                            counts['unknown_ml_internat'] += 1\n",
    "\n",
    "                    if ml_response['actual_result'] == u'success':\n",
    "                        counts['success_ml_back'] += 1\n",
    "                    elif ml_response['actual_result'] == u'block':\n",
    "                        counts['block_ml_back'] += 1\n",
    "                    elif ml_response['actual_result'] == u'unknown':\n",
    "                        counts['unknown_ml_back'] += 1\n",
    "\n",
    "                    # Exterior.\n",
    "                    if u'exterior_result' in ml_response:\n",
    "                        if ml_response['exterior_result'] == u'success':\n",
    "                            counts['success_ml_ext'] += 1\n",
    "                        elif ml_response['exterior_result'] == u'block':\n",
    "                            counts['block_ml_ext'] += 1\n",
    "                        elif ml_response['exterior_result'] == u'unknown':\n",
    "                            counts['unknown_ml_ext'] += 1\n",
    "                    else:\n",
    "                        if ml_response.get('actual_result') == u'success':\n",
    "                            counts['success_ml_ext'] += 1\n",
    "                        elif ml_response.get('result') == u'block':\n",
    "                            counts['block_ml_ext'] += 1\n",
    "                        elif (ml_response.get('actual_result') != u'success'\n",
    "                                  and ml_response.get('result') != u'block'):\n",
    "                            counts['unknown_ml_ext'] += 1\n",
    "                    # Interior.\n",
    "                    if interior_exam_enabled:\n",
    "                        if u'interior_result' in ml_response:\n",
    "                            if ml_response['interior_result'] == u'success':\n",
    "                                counts['success_ml_int'] += 1\n",
    "                            elif ml_response['interior_result'] == u'block':\n",
    "                                counts['block_ml_int'] += 1\n",
    "                            elif ml_response['interior_result'] == u'unknown':\n",
    "                                counts['unknown_ml_int'] += 1\n",
    "                        else:\n",
    "                            counts['unknown_ml_int'] += 1\n",
    "\n",
    "                    # Reasons.\n",
    "                    for reason_ext in constants.ReasonsNames.EXTERIOR:\n",
    "                        if isinstance(ml_response['incorrect'], list):\n",
    "                            if reason_ext in ml_response['incorrect']:\n",
    "                                counts['back_' + reason_ext.lower()] += 1\n",
    "                                if international_exam:\n",
    "                                    counts['back_' + reason_ext.lower()\n",
    "                                           + '_internat'] += 1\n",
    "                    if interior_exam_enabled:\n",
    "                        for reason_int in constants.ReasonsNames.INTERIOR:\n",
    "                            if isinstance(ml_response['incorrect'], list):\n",
    "                                if reason_int in ml_response['incorrect']:\n",
    "                                    counts['back_' + reason_int.lower()] += 1\n",
    "                                    if international_exam:\n",
    "                                        counts['back_' + reason_int.lower()\n",
    "                                               + '_internat'] += 1\n",
    "\n",
    "                    for prod_reason in config.PROD_BLOCK_REASON_SET:\n",
    "                        if isinstance(ml_response['incorrect'], list):\n",
    "                            if (prod_reason in ml_response['incorrect']\n",
    "                                    and ml_response.get('result') == u'block'):\n",
    "                                counts['prod_' + prod_reason.lower()] += 1\n",
    "                                if international_exam:\n",
    "                                    counts['prod_' + prod_reason.lower()\n",
    "                                           + '_internat'] += 1\n",
    "\n",
    "                if experiment and passed_to_assessor:\n",
    "                    if (ml_response['result'] == u'success'\n",
    "                            and record.get('final_resolution') == u'success'):\n",
    "                        counts['true_negative'] += 1\n",
    "                    elif (ml_response['result'] == u'block'\n",
    "                            and record.get('final_resolution') == u'success'):\n",
    "                        counts['false_positive'] += 1\n",
    "                    elif (ml_response['result'] == u'block'\n",
    "                            and record.get('final_resolution') == u'block'):\n",
    "                        counts['true_positive'] += 1\n",
    "                    elif (ml_response['result'] == u'success'\n",
    "                            and record.get('final_resolution') == u'block'):\n",
    "                        counts['false_negative'] += 1\n",
    "                    if record.get('final_resolution') not in config.RESOLUTION_SET:\n",
    "                        counts['total_exp_asr_unknown'] += 1\n",
    "\n",
    "                    if ml_response['result'] in config.RESOLUTION_SET:\n",
    "                        if record.get('final_resolution') == u'success':\n",
    "                            counts['success_assessor_exp_ml'] += 1\n",
    "                        elif record.get('final_resolution') == u'block':\n",
    "                            counts['block_assessor_exp_ml'] += 1\n",
    "                    elif ml_response['result'] == u'unknown':\n",
    "                        if record.get('final_resolution') == u'success':\n",
    "                            counts['success_assessor_exp_noml'] += 1\n",
    "                        elif record.get('final_resolution') == u'block':\n",
    "                            counts['block_assessor_exp_noml'] += 1\n",
    "                        counts['total_exp_ml_unknown'] += 1\n",
    "\n",
    "                        if (u'meta_scores' in ml_response.get('prediction', {})\n",
    "                                and record.get('final_resolution')\n",
    "                                in config.RESOLUTION_SET):\n",
    "                            ranking_score_list.append(\n",
    "                                ml_response['prediction']['meta_scores']\n",
    "                                ['meta_score_full_exam']\n",
    "                            )\n",
    "                            ranking_decision_list.append(\n",
    "                                record.get('final_resolution')\n",
    "                            )\n",
    "\n",
    "        for ranking_drop_prop in config.RANKING_DROP_PROPORTIONS:\n",
    "\n",
    "            ranking_drop_number = int(round(\n",
    "                ((ranking_drop_prop / 100.) * len(ranking_score_list))\n",
    "            ))\n",
    "\n",
    "            ranking_drop_exams_ml = heapq.nsmallest(\n",
    "                ranking_drop_number,\n",
    "                zip(ranking_score_list, ranking_decision_list),\n",
    "                key=lambda x: x[0]\n",
    "            )\n",
    "            ranking_drop_exams_random = random.sample(\n",
    "                zip(ranking_score_list, ranking_decision_list),\n",
    "                ranking_drop_number\n",
    "            )\n",
    "\n",
    "            for score, decision in ranking_drop_exams_ml:\n",
    "                if decision == u'success':\n",
    "                    counts['success_bottom_'\n",
    "                           + str(ranking_drop_prop)\n",
    "                           + '_ml'] += 1\n",
    "                elif decision == u'block':\n",
    "                    counts['block_bottom_'\n",
    "                           + str(ranking_drop_prop)\n",
    "                           + '_ml'] += 1\n",
    "\n",
    "            for score, decision in ranking_drop_exams_random:\n",
    "                if decision == u'success':\n",
    "                    counts['success_bottom_'\n",
    "                           + str(ranking_drop_prop)\n",
    "                           + '_random'] += 1\n",
    "                elif decision == u'block':\n",
    "                    counts['block_bottom_'\n",
    "                           + str(ranking_drop_prop)\n",
    "                           + '_random'] += 1\n",
    "        \n",
    "        yield Record(**counts)\n",
    "    \n",
    "    return exam_group_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:27.424937Z",
     "start_time": "2019-05-30T13:11:27.411636Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(job, date):\n",
    "    \n",
    "    city_country_table = (job.table(config.YT_CITY_COUNTRY_TABLE_PATH)\n",
    "                          .project('name', 'country', 'country_id'))\n",
    "    \n",
    "    qc_log_reduced = (\n",
    "        job\n",
    "        .table(''.join([config.YT_QUALITY_CONTROL_LOG_PREFIX, date]))\n",
    "        .filter(nf.custom(lambda x: x in config.RELEVANT_RQC_TYPES, 'qc_type'))\n",
    "        .join(city_country_table, by_left='city', by_right='name',\n",
    "              type='left', assume_small_right=True)\n",
    "        .project(ne.all(exclude=config.COLUMNS_TO_DROP_YT))\n",
    "        .groupby('qc_id')\n",
    "        .reduce(exam_group_reducer)\n",
    "    )\n",
    "    qc_log_mapped = (\n",
    "        qc_log_reduced\n",
    "        .map(create_exam_group_mapper(date), memory_limit=4096)\n",
    "    )\n",
    "\n",
    "    return qc_log_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:32.145544Z",
     "start_time": "2019-05-30T13:11:28.065126Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tables = []\n",
    "for d in dates.range_selector(START_DATE, FINISH_DATE).strip('{}').split(','):\n",
    "    tables.append(get_metrics(job, d))\n",
    "\n",
    "job.concat(*tables).put(config.YT_DASHBOARD_TABLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:11:33.095831Z",
     "start_time": "2019-05-30T13:11:33.090698Z"
    }
   },
   "outputs": [],
   "source": [
    "# job.flow_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:57:37.365100Z",
     "start_time": "2019-05-30T13:11:33.394042Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:14:33.854813Z",
     "start_time": "2019-05-29T15:14:32.663313Z"
    }
   },
   "outputs": [],
   "source": [
    "table = job.table(\n",
    "    config.YT_DASHBOARD_TABLE_PATH,\n",
    "    ignore_missing=True\n",
    ")\n",
    "\n",
    "df = table.read().as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T11:39:50.410107Z",
     "start_time": "2019-05-30T11:38:36.589209Z"
    }
   },
   "outputs": [],
   "source": [
    "df = cluster.read(config.YT_QUALITY_CONTROL_LOG_PREFIX\n",
    "                  + '2019-05-27').as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T11:39:51.523697Z",
     "start_time": "2019-05-30T11:39:50.850172Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rqc = df[df['qc_type'].apply(lambda x: x in config.RELEVANT_RQC_TYPES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T11:39:52.442299Z",
     "start_time": "2019-05-30T11:39:52.361207Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rqc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymlaas",
   "language": "python",
   "name": "pymlaas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
