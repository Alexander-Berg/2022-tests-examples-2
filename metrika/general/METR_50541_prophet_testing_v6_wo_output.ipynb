{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/garskova-d/metrika-analytics')\n",
    "\n",
    "from metr_utils.databases import clickhouse as ch\n",
    "from metr_utils.databases import yql_db\n",
    "from metr_utils.databases import yt_db\n",
    "from metr_utils.databases import mysql_db\n",
    "from metr_utils.databases import psql_db\n",
    "from metr_utils.configs import options\n",
    "from metr_utils.configs import get_vault_data\n",
    "from metr_utils import generate_query\n",
    "from metr_utils import plotly_graphs as pplot\n",
    "from metr_utils import pandas_helpers as ph\n",
    "from metr_utils import blackbox as bb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import builtins\n",
    "builtins.opts = options.args(get_robot_passes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore')\n",
    "sys.path.append(\"../\")\n",
    "from fbprophet import Prophet\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "import tqdm\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "from string import Template\n",
    "import csv\n",
    "import json\n",
    "import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "! mkdir raw_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(today_str, n_periods):\n",
    "\n",
    "    daily_today = pd.to_datetime(today_str)\n",
    "    daily_end_date = pd.to_datetime(today_str) - pd.Timedelta(days = n_periods)\n",
    "    daily_end_date_str = daily_end_date.strftime(\"%Y-%m-%d\")\n",
    "    daily_month_ago = daily_end_date - pd.Timedelta(days = 30)\n",
    "    daily_3_months_ago = daily_end_date - pd.Timedelta(days = 90)\n",
    "    daily_1_year_ago = daily_end_date - pd.Timedelta(days = 365)\n",
    "    daily_1_year_ago_str = daily_1_year_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    n_periods_future = 90\n",
    "    daily_month_ago_future = daily_today - pd.Timedelta(days = 30)\n",
    "    daily_3_months_ago_future = daily_today - pd.Timedelta(days = 90)\n",
    "    \n",
    "    daily_3_years_ago_future = daily_today - pd.Timedelta(days = 365*3)\n",
    "    daily_3_years_ago_future_str =  daily_3_years_ago_future.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    start_of_today_week = daily_today.to_period('W').start_time\n",
    "    \n",
    "    return daily_today, daily_end_date, daily_end_date_str, daily_month_ago, daily_3_months_ago, daily_1_year_ago, \\\n",
    "            daily_1_year_ago_str, n_periods_future, daily_month_ago_future, daily_3_months_ago_future, \\\n",
    "            daily_3_years_ago_future, daily_3_years_ago_future_str, start_of_today_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_counters_in_interface(months_ago = 6):\n",
    "    \n",
    "    global today_str\n",
    "    \n",
    "    active = f'''\n",
    "    select counter_id as CounterID, uniq(watch_id) as hits, uniq(date) as dates\n",
    "\n",
    "    from (\n",
    "    select\n",
    "            WatchID as watch_id,\n",
    "            UserID as user_id,\n",
    "            EventDate as date,\n",
    "            EventTime as datetime,\n",
    "            PassportUserID as passport_user_id,\n",
    "            URLPathHierarchy(cutQueryStringAndFragment(URL)) as hierarchy,\n",
    "            arrayMap(x -> if(x like '%/', substring(x, 1, length(x) - 1), x), URLPathHierarchy(cutQueryStringAndFragment(URL))) as hierarchy_filt,\n",
    "            replaceAll(hierarchy_filt[1], '//', '/') as main_url_path,\n",
    "            replaceAll(if(length(hierarchy_filt) = 1, hierarchy_filt[1], hierarchy_filt[2]), '//', '/') as url_path,\n",
    "            extract(if(extractURLParameter(URL, 'id') = '', extractURLParameter(URL, 'ids'), extractURLParameter(URL, 'id')), '[0-9]+') as counter_id\n",
    "        from hits_all\n",
    "        where EventDate >= toDate('{today_str}')-30*{months_ago}\n",
    "            and EventDate <= '{today_str}'\n",
    "            and CounterID = 24226447\n",
    "            and not DontCountHits\n",
    "            and not IsRobot\n",
    "            and url_path not in ('/inpage/visor-proto', '/inpage/visor-player', '/inpage')\n",
    "            and main_url_path in ('/stat', '/inpage', '/publishers', '/markedphones', '/legacy')\n",
    "            and counter_id != ''\n",
    "        settings max_rows_in_join = 0, max_bytes_in_join = 0, max_memory_usage = 100000000000)\n",
    "    group by counter_id\n",
    "    settings max_rows_in_join = 0, max_bytes_in_join = 0, max_memory_usage = 100000000000\n",
    "    format TSVWithNames\n",
    "    '''\n",
    "    df_counters_active = ch.get_df(active)\n",
    "    df_counters_active.CounterID = df_counters_active.CounterID.astype(int)\n",
    "    df_counters_active = df_counters_active[df_counters_active.CounterID > 0]\n",
    "    \n",
    "    return df_counters_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir counters_daily_fact\n",
    "def get_counter_visits(counter_id, add_additional_days = True):\n",
    "    \n",
    "    global daily_today\n",
    "    global today_str\n",
    "    global daily_3_years_ago_future\n",
    "    global daily_3_years_ago_future_str\n",
    "    \n",
    "    first_date_str = daily_3_years_ago_future_str\n",
    "    \n",
    "    filename = f'counters_daily_fact/daily_fact_{counter_id}.csv'\n",
    "    \n",
    "    daily_fact_сounter = pd.DataFrame()\n",
    "    \n",
    "    df_fact = pd.DataFrame()\n",
    "    \n",
    "    if add_additional_days == False and exists(filename):\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "\n",
    "        if exists(filename):\n",
    "\n",
    "            daily_fact_сounter = pd.read_csv(filename)\n",
    "            daily_fact_сounter['ds'] = pd.to_datetime(daily_fact_сounter['ds'])\n",
    "            first_date = daily_fact_сounter.ds.max() + pd.Timedelta(days = 1)\n",
    "            first_date_str = first_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        query = f'''\n",
    "        SELECT StartDate as ds, sum(Sign) AS y \n",
    "        FROM visits_all \n",
    "        WHERE CounterID = {counter_id}\n",
    "            and StartDate >= '{first_date_str}'\n",
    "            and StartDate <= '{today_str}'\n",
    "            and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "            and IsRobot = 0 \n",
    "        GROUP BY ds\n",
    "        HAVING y >= 0\n",
    "        FORMAT TabSeparatedWithNames\n",
    "        '''\n",
    "        for i in range(0, 3):\n",
    "            try:\n",
    "                df_fact = ch.get_df(query=query)\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "            \n",
    "        try:\n",
    "            df_fact['ds'] = pd.to_datetime(df_fact['ds'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if exists(filename):\n",
    "            df_fact = daily_fact_сounter.append(df_fact).reset_index(drop = True)\n",
    "\n",
    "        true_dates = pd.DataFrame(pd.date_range(df_fact.ds.min(), df_fact.ds.max()), columns = ['ds'])\n",
    "        df_fact = true_dates.merge(df_fact, how = 'left', left_on = 'ds', right_on = 'ds').fillna(0)\n",
    "        df_fact['y'] = df_fact['y'].astype(int)\n",
    "\n",
    "        if df_fact.shape[0] > 0:\n",
    "            df_fact.to_csv(filename, index = False, sep = ',')\n",
    "            #print('add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(A, F): \n",
    "    return np.mean(np.abs((A - F) / A)) * 100\n",
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "def smape_median(A, F):\n",
    "    return 100 * (2 * np.abs(F - A) / (np.abs(A) + np.abs(F))).median()\n",
    "def rmse(A, F):\n",
    "    return np.sqrt(mse(A, F))\n",
    "\n",
    "def smape_sum(A, F):\n",
    "    return (2 * np.abs(F - A) / (np.abs(A) + np.abs(F))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_holidays():\n",
    "    df_custom_holidays = pd.read_csv('~/arcadia/statbox/jam/actions/outer-action/analytics_keymetrics/research-3455/action/analyticskeymetrics/forecasting/models/holidays_list.csv')\n",
    "    holidays_list = df_custom_holidays.ds.tolist()\n",
    "    holidays_list_full = []\n",
    "    for day in df_custom_holidays.iterrows():\n",
    "        start = pd.to_datetime(day[1].ds) + timedelta(days=day[1].lower_window)\n",
    "        end = pd.to_datetime(day[1].ds) + timedelta(days=day[1].upper_window)\n",
    "        holidays_list_full = holidays_list_full + list(pd.date_range(start, end))\n",
    "    return df_custom_holidays, holidays_list_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(df_train,\n",
    "                    df_fact, \n",
    "                    capacity, \n",
    "                    freq, \n",
    "                    n_periods, \n",
    "                    date_end, \n",
    "                    n_periods_future,\n",
    "                    off_weekly = False, \n",
    "                    holidays_prior_scale = 8):\n",
    "    \n",
    "    global df_custom_holidays\n",
    "    global holidays_list_full\n",
    "    \n",
    "    # firstly we run model on train data, predict period that already happened in order to evaluate the result:\n",
    "    \n",
    "    train_df = df_train.copy() \n",
    "    \n",
    "    # obligatory columns for using logistic type of growth\n",
    "    # these restrictions are only for trend-compopent, not for seasonality:\n",
    "    \n",
    "    train_df['cap'] = df_train.y.max()*capacity # ceil is 1.1*max_visits, where max_visits is maximum visits of last 3 years\n",
    "    train_df['floor'] = 0 # floor for trend is 0\n",
    " \n",
    "    weekly_seasonality = 'auto' # add weekly_seasonality for everyone\n",
    "    \n",
    "    if off_weekly: # we turn off weekly seasonality on holidays\n",
    "        \n",
    "        weekly_seasonality = False\n",
    "        train_df['on_weekly'] = ~train_df.ds.isin(holidays_list_full) # special \"seasonality\" condition for holidays perios\n",
    "\n",
    "    all_params = ['additive', 'multiplicative'] # for future grid of best seasonlity type mode\n",
    "    prophet_errors = dict()\n",
    "    prophet_forecasts = dict()\n",
    "    prophet_featutes = {'additive':{}, 'multiplicative':{}}\n",
    "    \n",
    "    if train_df.shape[0] >= 365:  # add yearly seasonality if enough data (historical points more than 365)\n",
    "        yearly_seasonality = True\n",
    "    else:\n",
    "        yearly_seasonality = 'auto' # if historical points less than 365 let prophet reveal yearly sesonality automatically \n",
    "    \n",
    "    # sometimes prophet optimization failing and it results in fit problem. It may happens only for one type of \n",
    "    # seasonality. For example, only for additive sesonality mode. Therefore we trying to fit multiplicative mode. If it also\n",
    "    # fails we exclude counter from prediction\n",
    "    \n",
    "    fit_errors = 0 # counter for fit problem. 1 - only sesonality_mode failed. 2 - both failed (exclude from prediction)\n",
    "\n",
    "    for param in all_params:\n",
    "        \n",
    "        m = Prophet(seasonality_mode = param, \n",
    "                    growth = 'logistic', \n",
    "                    holidays = df_custom_holidays, \n",
    "                    holidays_prior_scale = holidays_prior_scale, \n",
    "                    weekly_seasonality=weekly_seasonality, \n",
    "                    yearly_seasonality = yearly_seasonality)\n",
    "\n",
    "        if off_weekly: # overwrite weekly seasonality if it holiday \n",
    "            m.add_seasonality(name='weekly', period=7, fourier_order=3, condition_name='on_weekly')\n",
    "    \n",
    "        try:\n",
    "            m.fit(train_df)\n",
    "            \n",
    "        except RuntimeError: # fit problem handling\n",
    "            fit_errors += 1\n",
    "            continue # swith to next seasonality mode if one failed               \n",
    "\n",
    "        future = m.make_future_dataframe(periods=n_periods, freq=freq) # freq is always daily\n",
    "        future['cap'] = df_train.y.max()*capacity # obligatory columns for using logistic type of growth \n",
    "        future['floor'] = 0 # obligatory columns for using logistic type of growth\n",
    "        if off_weekly: # we turn off weekly seasonality on holidays\n",
    "            future['on_weekly'] = ~future.ds.isin(holidays_list_full)\n",
    "        forecast = m.predict(future)\n",
    "        \n",
    "        # our forecast can be < 0 due to seasonality compopnent (trend restictions are taken into consideration above by \n",
    "        # 'cap' column. After final summarizing of trend+seasonality+holidays components we clip all negative values to 0:\n",
    "        \n",
    "        forecast['yhat_final'] = forecast['yhat'].clip(lower = 0) \n",
    "        forecast['yhat_final'] = forecast['yhat_final'].apply(lambda x: round(x, 0)) # visits are only integers, not float\n",
    "        \n",
    "        # write forecast and its rmses for additive and mupliplicative seasonality modes in order to choose the best one later\n",
    "        \n",
    "        prophet_forecasts[param] = forecast\n",
    "        prophet_errors[param] = rmse(df_fact[df_fact['ds']>date_end]['y'], forecast[forecast['ds']>date_end]['yhat_final'])\n",
    "        for el in features:\n",
    "            prophet_featutes[param][el] = getattr(m, el) # information about model\n",
    "     \n",
    "    if fit_errors == 2: # skip counter if prophet wasn't able to fit model eventually\n",
    "        return models_info\n",
    "\n",
    "    df_errors_tmp = pd.DataFrame(prophet_errors, index = ['prophet']).transpose()\n",
    "    \n",
    "    if df_errors_tmp.shape[0] != 0: \n",
    "        \n",
    "        # choose the best model (additive or multiplicative) by rmse and write it as best_model:\n",
    "        \n",
    "        best_prophet_model = df_errors_tmp\\\n",
    "                            [df_errors_tmp['prophet']==df_errors_tmp['prophet'].min()].reset_index()['index'].values[0]\n",
    "        forecast = prophet_forecasts[best_prophet_model]\n",
    "\n",
    "        error_mape = 0 # mape is useful for post-analysis\n",
    "        error_mape = mape(df_fact[df_fact['ds']>date_end]['y'], forecast[forecast['ds']>date_end]['yhat_final'])\n",
    "        \n",
    "        # add forecast to dataframe with facts:\n",
    "        \n",
    "        df_fact['prophet'] = np.nan \n",
    "        df_fact['prophet'].iloc[-n_periods:] = np.array(forecast[forecast['ds']>date_end]['yhat_final'])\n",
    "        \n",
    "        # models_info contains all the information on model:\n",
    "\n",
    "        models_info[counter_id] = {'fact': df_fact, 'forecast':forecast,\n",
    "                                   'prophet_features':prophet_featutes[best_prophet_model],\n",
    "                                   'error_prophet':df_errors_tmp\\\n",
    "                                           [df_errors_tmp['prophet']==df_errors_tmp.prophet.min()].prophet.values[0],\n",
    "                                   'mape':error_mape}\n",
    "\n",
    "    #print(models_info[counter_id]['prophet_features'])\n",
    "\n",
    "    scale = {'D':'D', 'W-MON':'W-MON', 'MS':'MS'} # freq is always daily, it's old code\n",
    "    \n",
    "    # dates for graph should also include zero points so we add them for post-analysis\n",
    "    \n",
    "    df_dates_for_graph = pd.DataFrame(pd.date_range(df_fact.ds.min(), df_fact.ds.max(), freq = scale[freq]), columns = ['ds'])\n",
    "    df_dates_for_graph = df_dates_for_graph.merge(df_fact, how = 'left')\n",
    "    df_dates_for_graph.y = df_dates_for_graph.y.fillna(0).astype(int)\n",
    "    \n",
    "    #print(pplot.one_df(df_dates_for_graph[['ds', 'y', 'prophet']].set_index('ds')))\n",
    "    \n",
    "    # next we fit model with best params and make forecast on real period:\n",
    "\n",
    "    train_df = df_fact[['ds','y']].copy()\n",
    "    train_df['cap'] = df_train.y.max()*capacity\n",
    "    train_df['floor'] = 0\n",
    "    \n",
    "    if train_df.shape[0] >= 365:  # Add yearly seasonality if enough data\n",
    "        yearly_seasonality = True\n",
    "    else:\n",
    "        yearly_seasonality = 'auto'\n",
    "        \n",
    "    weekly_seasonality = 'auto'\n",
    "    if off_weekly: \n",
    "        weekly_seasonality = False\n",
    "        train_df['on_weekly'] = ~train_df.ds.isin(holidays_list_full)  \n",
    "\n",
    "    m = Prophet(seasonality_mode = best_prophet_model,\n",
    "                growth = 'logistic',\n",
    "                holidays = df_custom_holidays,\n",
    "                holidays_prior_scale = holidays_prior_scale,\n",
    "                weekly_seasonality=weekly_seasonality,\n",
    "                yearly_seasonality = yearly_seasonality)\n",
    "\n",
    "    if off_weekly:\n",
    "        m.add_seasonality(name='weekly', period=7, fourier_order=3, condition_name='on_weekly')\n",
    "    \n",
    "    try:\n",
    "        m.fit(train_df)\n",
    "\n",
    "    except RuntimeError:\n",
    "        print('fit problem')\n",
    "\n",
    "    future = m.make_future_dataframe(periods=n_periods_future, freq=freq)\n",
    "    future['cap'] = df_train.y.max()*capacity\n",
    "    future['floor'] = 0\n",
    "    if off_weekly:\n",
    "            future['on_weekly'] = ~future.ds.isin(holidays_list_full)\n",
    "    forecast = m.predict(future)\n",
    "    \n",
    "    forecast['forecast'] = forecast['yhat']\n",
    "    forecast['forecast_upper'] = forecast['yhat_upper']\n",
    "    forecast['forecast_lower'] = forecast['yhat_lower']\n",
    "    \n",
    "    df_fact['yhat'] = np.nan\n",
    "    df_fact['yhat_lower'] = np.nan\n",
    "    df_fact['yhat_upper'] = np.nan\n",
    "    df_fact = df_fact.append(forecast[forecast['ds']>daily_today][['ds','yhat', 'yhat_lower', 'yhat_upper']])\n",
    "    df_fact = df_fact.merge(forecast[['ds','forecast', 'forecast_upper', 'forecast_lower']], \n",
    "                            left_on = 'ds', right_on = 'ds', how = 'left')\n",
    "\n",
    "    df_dates_for_graph = pd.DataFrame(pd.date_range(df_fact.ds.min(), df_fact.ds.max() + pd.Timedelta(days = n_periods_future), \n",
    "                                                                                                      freq = scale[freq]), \n",
    "                                                    columns = ['ds'])\n",
    "    df_dates_for_graph = df_dates_for_graph.merge(df_fact, how = 'left')\n",
    "    df_dates_for_graph.y = df_dates_for_graph.y.fillna(0).astype(int)\n",
    "    \n",
    "    #pplot.one_df(df_fact[['ds','y','yhat_final', 'yhat_lower_final', 'yhat_upper_final']].set_index('ds'))\n",
    "    \n",
    "    df_fact['future'] = (df_fact['ds']>daily_today).astype(int)\n",
    "    models_info[counter_id]['new_predict'] = df_fact\n",
    "\n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ticket_parser2_py3.api import v1 as tp2_api\n",
    "import vault_client\n",
    "import getpass\n",
    "\n",
    "def get_passport_secret():\n",
    "    curr_user = getpass.getuser()\n",
    "    bot_name = None\n",
    "    if curr_user == 'airflow':\n",
    "        bot_name = 'robot-conv-main'\n",
    "    yav = vault_client.instances.Production(rsa_login=bot_name)\n",
    "    # Версии нужно обновлять руками\n",
    "    passport_secret = yav.get_version('ver-01f1yyja32kmy44n0m49qkvr4n')['value']['client_secret']\n",
    "    # https://yav.yandex-team.ru/secret/sec-01dvn9t18n723mw4dvfne8r1fm/explore/versions\n",
    "    return passport_secret\n",
    "\n",
    "class TVM(object):\n",
    "    def __init__(self, self_client_id, self_secret, destinations, secret_key=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self_client_id (int): ID своего приложения, которое будет ходить с TVM в удаленный сервис\n",
    "            self_secret (str|dict): TVM секрет приложения\n",
    "            destinations (dict): Словарь {alias_name: service_id}, куда приложение будет ходить.\n",
    "        \"\"\"\n",
    "        self.settings = tp2_api.TvmApiClientSettings(\n",
    "            self_client_id=self_client_id,\n",
    "            self_secret=self_secret,\n",
    "            dsts=destinations,\n",
    "        )\n",
    "        self.client = tp2_api.TvmClient(self.settings)\n",
    "\n",
    "def get_tvm_client(secret):\n",
    "    if not secret:\n",
    "        secret = get_passport_secret()\n",
    "    tvm = TVM(\n",
    "        self_client_id=2017315,\n",
    "        self_secret=secret,\n",
    "        destinations={'metrika_api_test': 2000269, 'metrika_api': 2000270},\n",
    "        secret_key='client_secret',\n",
    "    )\n",
    "    return tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload(table, content, host):\n",
    "    '''Uploads data to table in ClickHouse'''\n",
    "    query_dict = {\n",
    "        'query': 'INSERT INTO ' + table + ' FORMAT TabSeparatedWithNames '\n",
    "    }\n",
    "    r = requests.post(host, data=content.encode('utf-8'), params=query_dict, auth=(options.args(get_robot_passes=True).ch_user, options.args(get_robot_passes=True).ch_pass))\n",
    "    result = r.text\n",
    "    if r.status_code == 200:\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(r.text)\n",
    "\n",
    "def upload_df(table, df, host):\n",
    "    return upload(table, df.to_csv(index=False, sep = '\\t'), host=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = '2022-07-25'\n",
    "n_periods = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_today, daily_end_date, daily_end_date_str, daily_month_ago, daily_3_months_ago, daily_1_year_ago, daily_1_year_ago_str, \\\n",
    "n_periods_future, daily_month_ago_future, daily_3_months_ago_future, daily_3_years_ago_future, daily_3_years_ago_future_str, \\\n",
    "start_of_today_week = get_dates(today_str, n_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['growth',\n",
    "'holidays_prior_scale',\n",
    "'n_changepoints',\n",
    "'changepoint_range',\n",
    "'yearly_seasonality',\n",
    "'weekly_seasonality',\n",
    "'daily_seasonality',\n",
    "'seasonality_mode',\n",
    "'seasonality_prior_scale',\n",
    "'changepoint_prior_scale',\n",
    "'holidays_prior_scale',\n",
    "'interval_width',\n",
    "'uncertainty_samples',\n",
    "'y_scale',\n",
    "'logistic_floor',\n",
    "'t_scale',\n",
    "'seasonalities',\n",
    "'country_holidays',\n",
    "'growth',\n",
    "'component_modes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom_holidays, holidays_list_full = get_current_holidays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор счетчиков с нуля - не запускать))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counters_active = get_active_counters_in_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_counters_active.to_csv('active_counters.csv', index = False, sep = ',')\n",
    "#df_counters_active.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'garskova.active_counters_in_interface'\n",
    "host = 'http://mtchvip01m.metrika.yandex.net:8123/'\n",
    "\n",
    "drop_table = '''drop TABLE if exists {table}'''.format(table=table)\n",
    "\n",
    "create_table = ''' CREATE TABLE {table} (CounterID UInt32, hits UInt64, dates UInt64) ENGINE = Log; '''.format(table = table)\n",
    "\n",
    "data = ch.get_df(query=drop_table, raw = True)\n",
    "data = ch.get_df(query=create_table, raw = True)\n",
    "\n",
    "upload_df(table, df_counters_active, host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_dfs = pd.DataFrame({'CounterID':[]})\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 10)):\n",
    "\n",
    "    query = f'''\n",
    "    select a.CounterID as CounterID from \n",
    "\n",
    "    (select distinct CounterID\n",
    "    from (SELECT CounterID, StartDate as ds, sum(Sign) as y\n",
    "    FROM visits_all\n",
    "    WHERE StartDate <= toDate('{today_str}')\n",
    "            and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "            and IsRobot = 0\n",
    "            and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "            and intHash32(CounterID) % 10 = {i}\n",
    "    group by CounterID, StartDate)\n",
    "    group by CounterID\n",
    "    ) as a\n",
    "\n",
    "    inner join \n",
    "\n",
    "    (SELECT CounterID, sum(Sign) as y\n",
    "    FROM visits_all\n",
    "    WHERE StartDate >= toDate('{today_str}') - 30 and StartDate <= toDate('{today_str}')\n",
    "            and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "            and IsRobot = 0\n",
    "            and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "            and intHash32(CounterID) % 10 = {i}\n",
    "    group by CounterID\n",
    "    having y > 0\n",
    "    ) as b\n",
    "\n",
    "    on a.CounterID = b.CounterID\n",
    "\n",
    "    inner join\n",
    "\n",
    "    (SELECT CounterID, sum(Sign) as y\n",
    "    FROM visits_all\n",
    "    WHERE StartDate <= toDate('{daily_end_date_str}')\n",
    "            and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "            and IsRobot = 0\n",
    "            and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "            and intHash32(CounterID) % 10 = {i}\n",
    "    group by CounterID\n",
    "    having y > 0\n",
    "    ) as c\n",
    "\n",
    "    on a.CounterID = c.CounterID\n",
    "\n",
    "    FORMAT TabSeparatedWithNames\n",
    "\n",
    "    '''\n",
    "    df_counters_sample = ch.get_df(query)\n",
    "    counters_dfs = counters_dfs.append(df_counters_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_dfs.CounterID = counters_dfs.CounterID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_dfs.to_csv('counters_active_and_filtered.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(counters).to_csv('counters.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор счетчиков из предыдущей итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = pd.read_csv('counters_active_and_filtered.csv')\n",
    "counters = counters.CounterID.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counters_exp = pd.Series(counters[0:36000])\n",
    "# counters_exp.to_csv('counters_exp.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = pd.read_csv('counters_exp.csv')['0'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = counters[0:14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "daily_fact = dict()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = 2) as executor:\n",
    "    futures = []\n",
    "    for counter_id in counters[:]:\n",
    "        try:\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_counter_visits, counter_id=counter_id, add_additional_days = True\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            print('error')\n",
    "            \n",
    "    #for counter_id, df in tqdm.tqdm(map(lambda x: x.result(), futures)):\n",
    "    #    pass\n",
    "    for el in tqdm.tqdm(futures):\n",
    "        el.result()\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Program finished in {finish_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters[1701:1704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проверочка\n",
    "filename = f'counters_daily_fact/daily_fact_87341212.csv'\n",
    "daily_fact_сounter = pd.read_csv(filename)\n",
    "daily_fact_сounter.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! mkdir counters_model_info\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "# РАСКОММЕНТИТЬ ЕСЛИ НАДО\n",
    "\n",
    "models_info = dict()\n",
    "result = dict()\n",
    "result_scales = {'fact_daily':{},'fact_weekly':{}, 'fact_monthly':{}}\n",
    "good_daily_counters = []\n",
    "\n",
    "for counter_id in tqdm.tqdm(counters):\n",
    "    #print(counter_id)\n",
    "    \n",
    "    #df_fact = get_counter_visits(counter_id)\n",
    "    #df_fact = daily_fact[counter_id]\n",
    "    \n",
    "    if counter_id not in result_scales['fact_daily'].keys():\n",
    "#         and daily_today not in final_result_scales['fact_daily'][counter_id]\\\n",
    "#         [~final_result_scales['fact_daily'][counter_id].y.isna()].ds.to_list()\n",
    "        \n",
    "        filename = f'counters_daily_fact/daily_fact_{counter_id}.csv'\n",
    "        df_fact = pd.read_csv(filename)\n",
    "        df_fact['ds'] = pd.to_datetime(df_fact['ds'])\n",
    "        df_fact = df_fact[df_fact['ds'] >= daily_3_years_ago_future]\n",
    "\n",
    "        df_train = df_fact[df_fact['ds']<=daily_end_date]\n",
    "        df_predict = df_fact[df_fact['ds']>daily_end_date]\n",
    "\n",
    "        true_dates = pd.DataFrame(pd.date_range(daily_end_date, daily_today), columns = ['date'])\n",
    "        df_true_dates = true_dates.merge(df_fact[df_fact['ds']>=daily_end_date], how = 'left', left_on = 'date', right_on = 'ds')[['date', 'y']].fillna(0).rename(columns = {'date':'ds'})\n",
    "        df_fact = df_fact[df_fact['ds']<daily_end_date].append(df_true_dates).reset_index(drop = True)\n",
    "        df_fact.y = df_fact.y.astype(int)\n",
    "\n",
    "        if df_train[df_train['ds']==daily_end_date].shape[0] == 0:\n",
    "            #print('add extra day')\n",
    "            df_train = df_train.append(pd.DataFrame({'ds':daily_end_date, 'y':[0]})).reset_index(drop = True).sort_values('ds')\n",
    "\n",
    "        if df_train.shape[0] <= 30:\n",
    "            pass\n",
    "            #print('too little history observations for any forecasts:', df_train.shape[0], 'days')\n",
    "        else:\n",
    "            result = make_prediction(df_train, \n",
    "                                     df_fact, \n",
    "                                     capacity = 1.1, \n",
    "                                     freq = 'D', \n",
    "                                     n_periods = n_periods, \n",
    "                                     date_end = daily_end_date, \n",
    "                                     off_weekly = True,\n",
    "                                     holidays_prior_scale = 8, \n",
    "                                     n_periods_future = n_periods_future)\n",
    "\n",
    "            if counter_id not in result:\n",
    "                continue\n",
    "\n",
    "            df_daily = result[counter_id]['new_predict'].copy()\n",
    "            daily_median = df_daily[(df_daily['ds']>=daily_month_ago_future) & (df_daily.ds <= daily_today)].y.median()\n",
    "            result_scales['fact_daily'][counter_id] = df_daily\n",
    "            df_daily.to_csv(f'counters_model_info/{counter_id}.csv', index = False, sep = ',')\n",
    "\n",
    "            if daily_median <= 10:\n",
    "                #print('too little data for daily forecast:', daily_median)\n",
    "                del result[counter_id]\n",
    "            else:\n",
    "                good_daily_counters.append(counter_id)\n",
    "\n",
    "\n",
    "final_result_daily = dict()\n",
    "final_result_scales = dict()\n",
    "final_result_daily = result.copy()\n",
    "final_result_scales = result_scales.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(result_scales['fact_daily'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result_daily = dict()\n",
    "# final_result_scales = dict()\n",
    "# final_result_daily = result.copy()\n",
    "# final_result_scales = result_scales.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_today in final_result_scales['fact_daily'][49434271][~final_result_scales['fact_daily'][49434271].y.isna()].ds.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_result_daily.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_daily[85546441]['new_predict'].tail(93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(final_result_scales['fact_daily'].keys())).to_csv('counters_fact_daily_exp_v6.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### нормализируем настоящий прогноз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_daily_predictions(daily_today, final_result_scales, n_days):\n",
    "    final_result_scales['new_predict_with_normalization'] = dict()\n",
    "    last_fact_date = daily_today\n",
    "    n = n_days\n",
    "    start_date = last_fact_date - pd.to_timedelta('%s days' % (n-1))\n",
    "    for counter_id in list(final_result_scales['fact_daily'].keys())[:]:\n",
    "        df = final_result_scales['fact_daily'][counter_id][['ds','y','prophet', 'yhat', 'yhat_upper','yhat_lower',\n",
    "                                                                          'forecast', 'forecast_lower', 'forecast_upper', ]]\n",
    "\n",
    "        fact_ag = df.set_index('ds').loc[start_date:last_fact_date, 'y'].mean()\n",
    "        forecast_agg = df.set_index('ds').loc[start_date:last_fact_date, 'forecast'].mean()\n",
    "        fact_forecast_diff = forecast_agg - fact_ag\n",
    "\n",
    "        df['yhat_new'] = df['yhat'] - fact_forecast_diff\n",
    "        df['yhat_upper_new'] = df['yhat_upper'] - fact_forecast_diff\n",
    "        df['yhat_lower_new'] = df['yhat_lower'] - fact_forecast_diff\n",
    "\n",
    "        df['yhat'] = df['yhat'].apply(lambda x: round(x, 0))\n",
    "        df['yhat_new'] = df['yhat_new'].apply(lambda x: round(x, 0))\n",
    "        df['yhat_upper_new'] = df['yhat_upper_new'].apply(lambda x: round(x, 0))\n",
    "        df['yhat_lower_new'] = df['yhat_lower_new'].apply(lambda x: round(x, 0))\n",
    "\n",
    "        df = df.set_index('ds')\n",
    "        df = df.clip(0)\n",
    "\n",
    "        #pplot.one_df(df[['y','yhat_new','yhat']])\n",
    "        #final_result_daily[counter_id]['new_predict_with_normalization'] = dict()\n",
    "        #final_result_daily[counter_id]['new_predict_with_normalization'] = df.clip(0).reset_index()\n",
    "        final_result_scales['new_predict_with_normalization'][counter_id] = dict()\n",
    "        final_result_scales['new_predict_with_normalization'][counter_id] = df.clip(0).reset_index()\n",
    "    return final_result_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_scales = normalize_daily_predictions(daily_today,\n",
    "                                                  final_result_scales, \n",
    "                                                  n_days = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter_id in list(final_result_daily.keys())[7111:7119]:\n",
    "    pplot.one_df(final_result_scales['new_predict_with_normalization'][counter_id][['ds','y', 'yhat_new']].set_index('ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_scales['new_predict_with_normalization'][37512210].tail(92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Убираем плохие прогнозы: свалившиеся в ноль. Убираем счетчики, у которых мало данных за последние 30 дней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_counters(counters_list, scale):\n",
    "    \n",
    "    global daily_month_ago_future\n",
    "    global daily_today\n",
    "    \n",
    "    counters = []\n",
    "    counters_bad = []\n",
    "    counters_bad_2 = []\n",
    "    for counter_id in counters_list[:]:\n",
    "        if scale == 'day':\n",
    "            df_try = final_result_scales['new_predict_with_normalization'][counter_id][['ds','y', 'yhat', 'yhat_new']]\n",
    "            if (df_try.yhat_new==0).sum()/(df_try.yhat_new).count()>=0.5 or sum(df_try.iloc[-10:,:]['yhat_new']==0) == 10:\n",
    "                counters_bad.append(counter_id)\n",
    "            elif df_try.set_index('ds').loc[daily_month_ago_future:daily_today,:'y'].median().values[0] <= 10:\n",
    "                counters_bad_2.append(counter_id)\n",
    "            else:\n",
    "                counters.append(counter_id)\n",
    "        if scale == 'week':\n",
    "            df_try = final_result_scales['fact_weekly'][counter_id][['ds','y','yhat_new']]\n",
    "            if (df_try[df_try['ds'] > daily_today].yhat_new==0).sum()/(df_try[df_try['ds'] > daily_today].yhat_new).count()>=0.5 or sum(df_try.iloc[-3:,:]['yhat_new']==0) == 3:\n",
    "                counters_bad.append(counter_id)\n",
    "            elif df_try.set_index('ds').loc[daily_month_ago_future:daily_today,:'y'].median().values[0] <= 10:\n",
    "                counters_bad_2.append(counter_id)\n",
    "            else:\n",
    "                counters.append(counter_id)\n",
    "            \n",
    "    return counters, counters_bad, counters_bad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_for_daily, counters_bad_zeros, counters_bad_median = filter_counters(list(final_result_daily.keys()), scale = 'day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counters_for_daily), len(counters_bad_zeros), len(counters_bad_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прогнозы неделя, месяц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weekly_monthly_predict(counters_for_weekly_monthly_predicts, print_info = False):\n",
    "    for counter_id in counters_for_weekly_monthly_predicts:\n",
    "        #df_daily = final_result_daily['new_predict_with_normalization'][counter_id]\n",
    "        df_daily = final_result_scales['new_predict_with_normalization'][counter_id].copy()\n",
    "        df_tmp_weekly = df_daily.copy()\n",
    "        df_tmp_monthly = df_daily.copy()\n",
    "\n",
    "        #print('weekly')\n",
    "\n",
    "        start_of_today_week = daily_today.to_period('W').start_time\n",
    "        end_of_today_week = pd.to_datetime(daily_today.to_period('W').end_time.strftime('%Y-%m-%d'))\n",
    "        last_day_of_forecast = df_tmp_weekly.ds.max()\n",
    "        start_of_last_week = last_day_of_forecast.to_period('W').start_time\n",
    "        end_of_last_week = pd.to_datetime(last_day_of_forecast.to_period('W').end_time.strftime('%Y-%m-%d'))\n",
    "\n",
    "        if daily_today != end_of_today_week:  \n",
    "            df_tmp_weekly = df_tmp_weekly.set_index('ds')\n",
    "            df_tmp_weekly.loc[start_of_today_week:daily_today, 'yhat_new'] = df_tmp_weekly.loc[start_of_today_week:daily_today, 'y']\n",
    "            df_tmp_weekly = df_tmp_weekly.reset_index()\n",
    "\n",
    "        df_tmp_weekly['ds'] = df_tmp_weekly['ds'].dt.to_period('W').apply(lambda x: x.start_time)\n",
    "        df_weekly = df_tmp_weekly.groupby('ds').agg({'y': lambda x: x.sum(skipna=False), \n",
    "                                                     'yhat_new': lambda x: x.sum(skipna = True), \n",
    "                                                     'yhat_upper_new': lambda x: x.sum(skipna = True), \n",
    "                                                     'yhat_lower_new': lambda x: x.sum(skipna = True),\n",
    "                                                    }).reset_index()\n",
    "\n",
    "        df_weekly.ds = pd.to_datetime(df_weekly.ds)\n",
    "\n",
    "        if last_day_of_forecast != end_of_last_week:\n",
    "            df_weekly = df_weekly[df_weekly['ds']<start_of_last_week]\n",
    "\n",
    "        weekly_median = df_weekly[(df_weekly['ds']>=daily_3_months_ago_future) & (df_weekly.ds <= daily_today)].y.median()\n",
    "        if weekly_median <= 10:\n",
    "            if print_info:\n",
    "                print('too little data for weekly forecast:', weekly_median)\n",
    "        elif df_weekly[df_weekly.ds <= daily_today].shape[0] < 10:\n",
    "            if print_info:\n",
    "                print('too little history observations for weekly forecast:', df_weekly[df_weekly.ds <= daily_today].shape[0])\n",
    "        else:\n",
    "            if print_info:\n",
    "                pplot.one_df(df_weekly.set_index('ds'))\n",
    "            result_scales['fact_weekly'][counter_id] = df_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_monthly_predict(final_result_scales['new_predict_with_normalization'].keys(), print_info = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counters_for_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_for_daily = counters_for_daily[:len(counters_for_daily) // 2]\n",
    "counters_weekly = set(final_result_scales['fact_weekly'].keys()) - set(counters_for_daily)\n",
    "counters_weekly_complex = counters_weekly - set(final_result_daily.keys())\n",
    "counters_weekly_simple = counters_weekly & set(final_result_daily.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counters_for_daily), len(counters_weekly_complex), len(counters_weekly_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### накладываем условия на weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_weekly_complex, counters_weekly_complex_bad_zeros, counters_weekly_complex_bad_median = filter_counters(list(counters_weekly_complex), 'week')\n",
    "print(len(counters_weekly_complex), len(counters_weekly_complex_bad_zeros), len(counters_weekly_complex_bad_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counters_for_daily), len(counters_weekly_simple), len(counters_weekly_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "PRAGMA AnsiInForEmptyOrNullableItemsCollections;\n",
    "SELECT\n",
    "    counter_id\n",
    "FROM hahn.`home/metrika/data-transfer/prod/mtacs-metrikamain/conv_main_counters`\n",
    "where status = 'Active';\n",
    "'''\n",
    "counters_active_mysql = yql_db.get_df(query=query, token=opts.yql_oauth, add_default_header=True)\n",
    "counters_active_mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counters_for_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_for_daily = list(set(counters_for_daily) & set(counters_active_mysql.counter_id.to_list()))\n",
    "counters_weekly_complex = list(set(counters_weekly_complex) & set(counters_active_mysql.counter_id.to_list()))\n",
    "counters_weekly_simple = list(set(counters_weekly_simple) & set(counters_active_mysql.counter_id.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir counters_three_types_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(counters_for_daily).to_csv('counters_three_types_exp/counters_for_daily.csv', index = False, sep = ',')\n",
    "pd.Series(counters_weekly_complex).to_csv('counters_three_types_exp/counters_weekly_complex.csv', index = False, sep = ',')\n",
    "pd.Series(counters_weekly_simple).to_csv('counters_three_types_exp/counters_weekly_simple.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counters_for_daily), len(counters_weekly_simple), len(counters_weekly_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3835 + 4050 + 2591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заливаем данные на тестинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm = get_tvm_client(None)\n",
    "ticket = tvm.client.get_service_ticket_for('metrika_api_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### заливка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_daily_data_for_uploading(counter_id):\n",
    "    \n",
    "    global daily_1_year_ago_str\n",
    "    global daily_today\n",
    "    \n",
    "    filename = f'counters_predict/{counter_id}.json'\n",
    "    \n",
    "    df_beta = final_result_scales['new_predict_with_normalization'][counter_id][['ds','y','yhat_new','yhat_upper_new','yhat_lower_new']]\n",
    "    df_beta = df_beta[df_beta['ds']>= daily_1_year_ago_str]\n",
    "    df_beta['future'] = (df_beta['ds']>daily_today).astype(int)\n",
    "    df_beta = df_beta.set_index('ds')\n",
    "    df_beta.loc[daily_today,'future'] = 2\n",
    "    df_beta.loc[daily_today,'yhat_new'] = df_beta.loc[daily_today,'y']\n",
    "    df_beta = df_beta.reset_index()\n",
    "    dates = df_beta.query('future!=1').ds.apply(lambda x: x.strftime('%Y.%m.%d')).to_list()\n",
    "    values = list(map(int, df_beta.query('future!=1').y.to_list()))\n",
    "    pred_dates = df_beta.query('future!=0').ds.apply(lambda x: x.strftime('%Y.%m.%d')).to_list()\n",
    "    pred_values = list(map(int, df_beta.query('future!=0').yhat_new.to_list()))\n",
    "    pred_lower = list(map(int, df_beta.query('future==1').yhat_lower_new.to_list()))\n",
    "    pred_upper = list(map(int, df_beta.query('future==1').yhat_upper_new.to_list()))\n",
    "    recs = {'counter_id': counter_id,\n",
    "                    'dates':dates,\n",
    "                    'values':values, \n",
    "                    'pred_dates':pred_dates,\n",
    "                    'pred_values':pred_values,\n",
    "                    'pred_lower':pred_lower,\n",
    "                    'pred_upper':pred_upper,\n",
    "                    \"name\": \"Факт\",\n",
    "                    \"pred_name\": \"План\",\n",
    "                    \"lower_name\": \"Нижняя граница\",\n",
    "                    \"upper_name\": \"Верхняя граница\"}\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(recs, f)\n",
    "    \n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -r counters_predict\n",
    "# ! mkdir counters_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_weekly_data_for_uploading(counter_id):\n",
    "    \n",
    "    global daily_1_year_ago_str\n",
    "    global daily_today\n",
    "    \n",
    "    filename = f'counters_predict/{counter_id}.json'\n",
    "    \n",
    "    df_beta = final_result_scales['fact_weekly'][counter_id][['ds','y','yhat_new','yhat_upper_new','yhat_lower_new']]\n",
    "    df_beta = df_beta[df_beta['ds']>=daily_1_year_ago_str]\n",
    "    end_of_today_week = pd.to_datetime(daily_today.to_period('W').end_time.strftime('%Y-%m-%d'))\n",
    "    if end_of_today_week != daily_today:\n",
    "        weeks = 1\n",
    "    else:\n",
    "        weeks = 0\n",
    "    df_beta['future'] = (df_beta['ds']>start_of_today_week-timedelta(weeks=weeks)).astype(int)\n",
    "    df_beta = df_beta.set_index('ds')\n",
    "    df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'future'] = 2\n",
    "    df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'yhat_new'] = df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'y']\n",
    "    df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'yhat_upper_new'] = df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'y']\n",
    "    df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'yhat_lower_new'] = df_beta.loc[start_of_today_week-timedelta(weeks=weeks),'y']\n",
    "    \n",
    "    diff_upper = df_beta.loc[start_of_today_week + pd.Timedelta(weeks = weeks),:]['yhat_upper_new']-df_beta.loc[start_of_today_week + pd.Timedelta(weeks = weeks),:]['yhat_new']\n",
    "    diff_lower = df_beta.loc[start_of_today_week + pd.Timedelta(weeks = weeks),:]['yhat_new']-df_beta.loc[start_of_today_week + pd.Timedelta(weeks = weeks),:]['yhat_lower_new']\n",
    "    df_beta.loc[start_of_today_week,'yhat_upper_new'] = df_beta.loc[start_of_today_week,'yhat_new']+diff_upper\n",
    "    df_beta.loc[start_of_today_week,'yhat_lower_new'] = df_beta.loc[start_of_today_week,'yhat_new']-diff_lower\n",
    "    \n",
    "    df_beta = df_beta.clip(0)\n",
    "    df_beta = df_beta.reset_index()\n",
    "    #pplot.one_df(df_beta.set_index('ds'))\n",
    "    dates = df_beta.query('future!=1').ds.apply(lambda x: x.strftime('%Y.%m.%d')).to_list()\n",
    "    values = list(map(int, df_beta.query('future!=1').y.to_list()))\n",
    "    pred_dates = df_beta.query('future!=0').ds.apply(lambda x: x.strftime('%Y.%m.%d')).to_list()\n",
    "    pred_values = list(map(int, df_beta.query('future!=0').yhat_new.to_list()))\n",
    "    pred_lower = list(map(int, df_beta.query('future!=0').yhat_lower_new.to_list()))\n",
    "    pred_upper = list(map(int, df_beta.query('future!=0').yhat_upper_new.to_list()))\n",
    "    recs = {'counter_id': counter_id,\n",
    "                    'dates':dates,\n",
    "                    'values':values, \n",
    "                    'pred_dates':pred_dates,\n",
    "                    'pred_values':pred_values,\n",
    "                    'pred_lower':pred_lower,\n",
    "                    'pred_upper':pred_upper,\n",
    "                    \"name\": \"Факт\",\n",
    "                    \"pred_name\": \"План\",\n",
    "                    \"lower_name\": \"Нижняя граница\",\n",
    "                    \"upper_name\": \"Верхняя граница\"}\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(recs, f)\n",
    "    \n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def push_data(recs, counter_id, urls_uploaded, report_ids):\n",
    "    \n",
    "    global today_str \n",
    "    \n",
    "    i = '{i}'\n",
    "    url_tmpl = 'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/create?type={report}&fromDate={start_date}&toDate={end_date}'\n",
    "    r = requests.post(\n",
    "        url=url_tmpl.format(counter_id = counter_id, start_date = today_str, end_date = today_str, report = 'prediction'),\n",
    "        headers={'X-Ya-Service-Ticket': ticket},\n",
    "        files = {'file': json.dumps(recs)}, verify=False\n",
    "    )\n",
    "    print(r.status_code, r.text[0:300])\n",
    "    urls_uploaded.append(f'https://semenova2kate-metr-48422.dev.metrika.yandex.ru/experimental/{i}/?id={counter_id}')\n",
    "    report_ids.append(eval(r.text)['report_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(counters, scale = 'day'):\n",
    "    \n",
    "    urls_uploaded = []\n",
    "    report_ids = []\n",
    "    for counter_id in counters:\n",
    "        if scale == 'day':\n",
    "            recs = prepare_daily_data_for_uploading(counter_id)\n",
    "        if scale == 'week':\n",
    "            recs = prepare_weekly_data_for_uploading(counter_id)\n",
    "        push_data(recs, counter_id, urls_uploaded, report_ids)\n",
    "\n",
    "    return urls_uploaded, report_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_uploaded_daily, report_ids_daily = upload_data(counters_for_daily, 'day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_uploaded_weekly_complex, report_ids_weekly_complex = upload_data(counters_weekly_complex, 'week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_uploaded_weekly_simple, report_ids_weekly_simple = upload_data(counters_weekly_simple, 'week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_uploaded_urls(urls_uploaded, report_ids):\n",
    "    urls_uploaded_final = []\n",
    "    i = report_ids[0]\n",
    "    for url in urls_uploaded:\n",
    "        print(url.format(i = i))\n",
    "        urls_uploaded_final.append(url.format(i = i))\n",
    "        i+=1\n",
    "    return urls_uploaded_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = show_uploaded_urls(urls_uploaded_daily, report_ids_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = show_uploaded_urls(urls_uploaded_weekly_complex, report_ids_weekly_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l3 = show_uploaded_urls(urls_uploaded_weekly_simple, report_ids_weekly_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(l1).to_csv('counters_three_types_exp/urls_daily.csv', index = False, sep = ',')\n",
    "pd.Series(l2).to_csv('counters_three_types_exp/urls_weekly_complex.csv', index = False, sep = ',')\n",
    "pd.Series(l3).to_csv('counters_three_types_exp/urls_weekly_simple.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### удаление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = report_ids_daily[0]\n",
    "#i = 6375\n",
    "for counter_id in counters_for_daily:\n",
    "        url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/{i}'\n",
    "        r = requests.delete(\n",
    "            url=url_tmpl,\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            verify=False\n",
    "        )\n",
    "        print(url_tmpl)\n",
    "        print(r.text[0:300])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = report_ids_weekly_complex[0]\n",
    "#i = 2725\n",
    "for counter_id in counters_weekly_complex:\n",
    "        url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/{i}'\n",
    "        r = requests.delete(\n",
    "            url=url_tmpl,\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            verify=False\n",
    "        )\n",
    "        print(url_tmpl)\n",
    "        print(r.status_code, r.text[0:300])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = report_ids_weekly_simple[0]\n",
    "#i = 971\n",
    "for counter_id in counters_weekly_simple:\n",
    "        url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/{i}'\n",
    "        r = requests.delete(\n",
    "            url=url_tmpl,\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            verify=False\n",
    "        )\n",
    "        print(url_tmpl)\n",
    "        print(r.status_code, r.text[0:300])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''\n",
    "select * \n",
    "from experimental_reports\n",
    "where type = 'prediction' and status = 'Active'\n",
    "'''\n",
    "\n",
    "current_reports = mysql_db.get_df(\n",
    "    query = q, \n",
    "    host = 'metrika-main.haproxy-test.metrika.yandex.net')\n",
    "current_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30024\n",
    "for counter_id in current_reports.counter_id.to_list():\n",
    "        url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/{i}'\n",
    "        r = requests.delete(\n",
    "            url=url_tmpl,\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            verify=False\n",
    "        )\n",
    "        print(url_tmpl)\n",
    "        print(r.status_code, r.text[0:300])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### сохранение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### сохраняем urls на yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_uploaded_total = l1 + l2 + l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_to_yt = pd.DataFrame({'counter_id':list(map(lambda x: int(re.findall(r'id=(\\d+)', x)[0]), urls_uploaded_total)),\n",
    "                'url':urls_uploaded_total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.create_folder(f'//home/metrica-analytics/garskova-d/predictions/{today_str}')\n",
    "yt_db.create_folder(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.write_to_yt(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/result/urls', \n",
    "                 df_urls_to_yt,\n",
    "                 append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### сохрянаем залитые инфо на yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''\n",
    "select * \n",
    "from experimental_reports\n",
    "where type = 'prediction' and status = 'Active'\n",
    "'''\n",
    "\n",
    "loaded_reports_df = mysql_db.get_df(\n",
    "    query = q, \n",
    "    host = 'metrika-main.haproxy-test.metrika.yandex.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.write_to_yt(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/mysql_and_info', \n",
    "                 loaded_reports_df[['report_id','counter_id','file_name']],\n",
    "                 append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сохраняем залитые предикты на yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.create_folder(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/daily_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predicts = 'counters_predict'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "onlyfiles = [f for f in listdir(path_predicts) if isfile(join(path_predicts, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame()\n",
    "\n",
    "for path in tqdm.tqdm(onlyfiles):\n",
    "    with open(f'counters_predict/{path}') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    df_tmp = df_tmp.append(pd.DataFrame({'counter_id':data['counter_id'],'rec':str(data)}, index = [0]))\n",
    "df_tmp = df_tmp.reset_index(drop = True)\n",
    "\n",
    "yt_db.write_to_yt(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/result/counters_result_recs', \n",
    "             df_tmp,\n",
    "             append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### сохраняем daily_fact на yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predicts = 'counters_daily_fact'\n",
    "\n",
    "onlyfiles = [f for f in listdir(path_predicts) if isfile(join(path_predicts, f))]\n",
    "\n",
    "df_daily_facts_to_yt = pd.DataFrame()\n",
    "for path in tqdm.tqdm(onlyfiles):\n",
    "    data = pd.read_csv(f'{path_predicts}/{path}').set_index('ds').to_json()\n",
    "    counter_id = int(re.findall(r'\\d+', path)[0])\n",
    "    df_tmp = pd.DataFrame({'counter_id':counter_id,'df':data}, index = [0])\n",
    "    df_daily_facts_to_yt = df_daily_facts_to_yt.append(df_tmp)\n",
    "df_daily_facts_to_yt = df_daily_facts_to_yt.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.write_to_yt(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/daily_fact/counters_daily_fact_df', \n",
    "                 df_daily_facts_to_yt,\n",
    "                 append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### сохраняем тип счетчика из экспа на yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(counters_for_daily).to_csv('counters_three_types_exp/counters_for_daily.csv', index = False, sep = ',')\n",
    "pd.Series(counters_weekly_complex).to_csv('counters_three_types_exp/counters_weekly_complex.csv', index = False, sep = ',')\n",
    "pd.Series(counters_weekly_simple).to_csv('counters_three_types_exp/counters_weekly_simple.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(pd.Series(counters_for_daily), columns = ['counter_id'])\n",
    "df1['type'] = 'daily'\n",
    "df2 = pd.DataFrame(pd.Series(counters_weekly_complex), columns = ['counter_id'])\n",
    "df2['type'] = 'weekly_complex'\n",
    "df3 = pd.DataFrame(pd.Series(counters_weekly_simple), columns = ['counter_id'])\n",
    "df3['type'] = 'weekly_simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types = df1.append(df2).append(df3).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_db.write_to_yt(f'//home/metrica-analytics/garskova-d/predictions/{today_str}/counters_exp_type', \n",
    "                 df_types,\n",
    "                 append = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "\n",
    "insert into hahn.`home/metrica-analytics/garskova-d/predictions/{today_str}/visits` with truncate\n",
    "\n",
    "SELECT\n",
    "    counter_id,\n",
    "    probable_host,\n",
    "    main_country_traff,\n",
    "    tier,\n",
    "    source,\n",
    "    visits_last_month,\n",
    "    users_last_month,\n",
    "    is_rsya,\n",
    "    client_id,\n",
    "    clicks_visits,\n",
    "FROM hahn.`//home/metrica-analytics/reports/metrika_marketing_cube/marketing_cube_final/m/latest`\n",
    "'''\n",
    "run = yql_db.get_df(query=query, token=opts.yql_oauth, add_default_header=True)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "INSERT INTO hahn.`home/metrica-analytics/garskova-d/predictions/{today_str}/result/urls_and_visits` WITH TRUNCATE \n",
    "\n",
    "select t.*,\n",
    "        '((' || url || ' link))' as url_wiki\n",
    " from (\n",
    "\n",
    "SELECT\n",
    "    *\n",
    "FROM hahn.`home/metrica-analytics/garskova-d/predictions/{today_str}/result/urls` as a\n",
    "\n",
    "left join \n",
    "(select * from \n",
    "hahn.`home/metrica-analytics/garskova-d/predictions/{today_str}/visits`) as b\n",
    "\n",
    "on a.counter_id = b.counter_id\n",
    "\n",
    "left join\n",
    "\n",
    "(select * from \n",
    "hahn.`home/metrica-analytics/garskova-d/predictions/{today_str}/counters_exp_type`) as c\n",
    "\n",
    "on a.counter_id = c.counter_id) as t\n",
    "\n",
    "order by visits_last_month desc;\n",
    "'''\n",
    "run = yql_db.get_df(query=query, token=opts.yql_oauth, add_default_header=True)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### загрузка предиктов с yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#today_str = '2022-05-21'\n",
    "query = f'''\n",
    "PRAGMA AnsiInForEmptyOrNullableItemsCollections;\n",
    "SELECT *\n",
    "FROM hahn.`//home/metrica-analytics/garskova-d/predictions/{today_str}/result/counters_result_recs`\n",
    ";\n",
    "'''\n",
    "df_from_yql = yql_db.get_df(query=query, token=opts.yql_oauth, add_default_header=True)\n",
    "df_from_yql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_yql = df_from_yql[df_from_yql.counter_id.isin(counters_active_mysql.counter_id.to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_yql.rec.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заливаем данные с yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_data_from_yt(df_from_yql):\n",
    "    \n",
    "    global today_str \n",
    "    \n",
    "    urls_uploaded = []\n",
    "    \n",
    "    for el in df_from_yql.iterrows():\n",
    "        counter_id = el[1].counter_id\n",
    "        recs = eval(el[1].rec)\n",
    "    \n",
    "        url_tmpl = 'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/create?type={report}&fromDate={start_date}&toDate={end_date}'\n",
    "        r = requests.post(\n",
    "            url=url_tmpl.format(counter_id = counter_id, start_date = today_str, end_date = today_str, report = 'prediction'),\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            files = {'file': json.dumps(recs)}, verify=False\n",
    "        )\n",
    "\n",
    "        i = eval(r.text)['report_id']\n",
    "        print(r.status_code, r.text[0:300])\n",
    "        urls_uploaded.append(f'https://semenova2kate-metr-48422.dev.metrika.yandex.ru/experimental/{i}/?id={counter_id}')\n",
    "\n",
    "    return urls_uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_uploaded_total = push_data_from_yt(df_from_yql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(urls_uploaded_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(urls_uploaded_total).values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(urls_uploaded_total).to_csv('urls_uploaded_total_26_07_2022.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_yql[df_from_yql['counter_id']==65746927]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "65746927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_yql.iloc[:813]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### удаляем, если что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = report_ids_daily[0]\n",
    "i = 18881\n",
    "for counter_id in df_from_yql.counter_id.to_list():\n",
    "        url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/{i}'\n",
    "        r = requests.delete(\n",
    "            url=url_tmpl,\n",
    "            headers={'X-Ya-Service-Ticket': ticket},\n",
    "            verify=False\n",
    "        )\n",
    "        print(url_tmpl)\n",
    "        print(r.status_code, r.text[0:300])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Графички вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pplot.one_df(final_result_scales['fact_weekly'][6596923][['ds','y','yhat_new','yhat_upper_new','yhat_lower_new']].set_index('ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pplot.one_df(final_result_scales['new_predict_with_normalization'][66791488][['ds','y','yhat_new','yhat_upper_new','yhat_lower_new']].set_index('ds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://6xboyh55epm4o674.sas.yp-c.yandex.net:8080/internal/v1/counter/57008650/experimental_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /internal/v1/counter/{counterId}/experimental_report/{report_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for counter_id in list(final_result_daily.keys()):\n",
    "#     url_tmpl = f'https://internalapi.test.metrika.yandex.net/internal/v1/counter/{counter_id}/experimental_report/1210'\n",
    "#     r = requests.get(\n",
    "#         url=url_tmpl,\n",
    "#         headers={'X-Ya-Service-Ticket': ticket},\n",
    "#         verify=False\n",
    "#     )\n",
    "#     print(url_tmpl)\n",
    "#     print(r.text)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old counters selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f'''\n",
    "# select a.CounterID as CounterID from \n",
    "\n",
    "# (select distinct CounterID\n",
    "# from (SELECT CounterID, StartDate as ds, sum(Sign) as y\n",
    "# FROM visits_all\n",
    "# WHERE StartDate <= toDate('{today_str}')\n",
    "#         and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "#         and IsRobot = 0\n",
    "#         and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "# group by CounterID, StartDate)\n",
    "# group by CounterID) as a\n",
    "\n",
    "# inner join \n",
    "\n",
    "# (SELECT CounterID, sum(Sign) as y\n",
    "# FROM visits_all\n",
    "# WHERE StartDate >= toDate('{today_str}') - 30 and StartDate <= toDate('{today_str}')\n",
    "#         and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "#         and IsRobot = 0\n",
    "#         and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "# group by CounterID\n",
    "# having y > 0) as b\n",
    "\n",
    "# on a.CounterID = b.CounterID\n",
    "\n",
    "# inner join\n",
    "\n",
    "# (SELECT CounterID, sum(Sign) as y\n",
    "# FROM visits_all\n",
    "# WHERE StartDate <= toDate('{daily_end_date_str}')\n",
    "#         and ClientIPNetwork != 'YANDEX_SERVER' \n",
    "#         and IsRobot = 0\n",
    "#         and CounterID global in (select CounterID from garskova.active_counters_in_interface)\n",
    "# group by CounterID\n",
    "# having y > 0) as c\n",
    "\n",
    "# on a.CounterID = c.CounterID\n",
    "\n",
    "# FORMAT TabSeparatedWithNames\n",
    "# '''\n",
    "# df_counters_sample = ch.get_df(query)\n",
    "# df_counters_sample\n",
    "# #counters = df_counters_sample.CounterID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
